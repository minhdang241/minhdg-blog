{
  
    
        "post0": {
            "title": "Word Embeddings (Word2Vec) [Part1]",
            "content": "The full notebook is available here_%5BPart_1%5D.ipynb). . Word Embeddings Overview . Word embeddings are dense vectors of real numbers, one for each word in the vocabulary, which is the collection of words extracted from the dataset. . There are many ways to represent a word on a computer. For example, we can use ASCII code. Yet, it only tells what the word is, not its meaning. Another option is to use a one-hot vector to represent a word, in which we put the number 1 in the location of the represented word. However, using a one-hot vector has 2 main drawbacks. First of all, the vector is huge and sparse. The size of the vector is the same as the size of the vocabulary. The vector is sparse since there is only one position that has a non-zero value. Next, it treats all words independently, with no relation to each other. Technically said it does not provide any notion of similarity between words. . Take an example from Pytorch documentation: . Suppose we are building a language model. Suppose we have seen the sentences. . The mathematician ran to the store. | The physicist ran to the store. | The mathematician solved the open problem. | . In the training data, suppose we get the sentence never seen before:* The physicist solved the open problem.Our language model might be doing OK on this sentence. But it&#39;s better if we can use the following facts: . We have seen the mathematician and physicist in the same role in the sentence. As a result, they can have a semantic relation somehow. | We have seen mathematicians in the same role in this new unseen sentence as we are now seeing physicists.That infers the physicist is a good fit in the new unseen sentence. That&#39;s what we mean by semantic similarity. That relies on the assumption that:words appearing in similar contexts are related to each other semantically To encode the similarity between words we can think up some semantic attributes. We then put those attributes in the vector and give a score for each. We give high scores for shared attributes and low scores for the counterparts. We can measure the similarity between two word vectors using the dot product. As a result, similar words will have a similarity score near 1 and different words will have a similarity score near zero. . | Since thinking of the semantic attributes is hard and manually intensive, we can let them be the parameters in the network and be updated during training. One drawback of doing this way is the attribute scores are not interpretable. That is, we do not know what is attribute that a specific score represents. . In summary, word embeddings are a representation of the semantics of a word, efficiently encoding semantic information that might be relevant to the task at hand . To read more about word embeddings in Pytorch, click here . Word2Vec . Word2Vec is one the approaches to develop a word embedding. There are two algorithms used in Word2Vec: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the probability of context words from a center word. In this post, we will try to implement both. . Continuous Bag-of-words (CBOW) (Pytorch) . The algorithm aims to predict a center word give the surrounding context in terms of word vectors. For example, given a sentence &quot;The cat jumped over the puddle&quot;, the algorithm treats {&quot;The&quot;, &quot;cat&quot;, &quot;over&quot;, &quot;the&quot;, &quot;puddle&quot;} as context words and {&quot;jumped&quot;} as the center word. The objective is to generate the center word from context words. . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # reproduction purpose torch.manual_seed(1) . &lt;torch._C.Generator at 0x7ff4203ac150&gt; . CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right EMBEDDING_DIM = 10 HIDDEN_DIM = 128 raw_text = &quot;&quot;&quot;We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.&quot;&quot;&quot;.split() # By deriving a set from `raw_text`, we deduplicate the array vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(raw_text) - 2): context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]] target = raw_text[i] data.append((context, target)) class CBOW(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): super(CBOW, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(embedding_dim, hidden_dim) self.linear2 = nn.Linear(hidden_dim, vocab_size) def forward(self, inputs): out = self.embeddings(inputs) out = torch.sum(out, dim=0).view(1, -1) out = F.relu(self.linear1(out)) out = F.log_softmax(self.linear2(out), dim=-1) return out # create your model and train. here are some functions to help you make # the data ready for use by your module def make_context_vector(context, word_to_ix): idxs = [word_to_ix[w] for w in context] return torch.tensor(idxs, dtype=torch.long) # Training losses = [] loss_function = nn.NLLLoss() model = CBOW(vocab_size, EMBEDDING_DIM, HIDDEN_DIM) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = 0 for context, target in data: context_idxs = make_context_vector(context, word_to_ix) model.zero_grad() log_probs = model(context_idxs) loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long)) loss.backward() optimizer.step() total_loss += loss.item() losses.append(total_loss) print(losses) print(model.embeddings.weight[word_to_ix[&#39;spirits&#39;]]) . [238.0529305934906, 233.41328835487366, 228.94981503486633, 224.64973831176758, 220.50258708000183, 216.49783158302307, 212.62398993968964, 208.87176704406738, 205.23141360282898, 201.69729340076447] tensor([-0.7098, -0.6179, -0.3807, 2.3069, -0.7957, 1.4458, 0.6856, 2.1891, -0.2936, 0.5549], grad_fn=&lt;SelectBackward&gt;) . Skip-gram (Pytorch) . CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right EMBEDDING_DIM = 10 HIDDEN_DIM = 128 raw_text = &quot;&quot;&quot;We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.&quot;&quot;&quot;.split() # By deriving a set from `raw_text`, we deduplicate the array vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(raw_text) - 2): context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]] target = raw_text[i] for value in context: sample = (target, value) data.append(sample) class SkipGram(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): super(SkipGram, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(embedding_dim, hidden_dim) self.linear2 = nn.Linear(hidden_dim, vocab_size) def forward(self, inputs): out = self.embeddings(inputs) out = F.relu(self.linear1(out)) out = F.log_softmax(self.linear2(out), dim=-1) return out # create your model and train. here are some functions to help you make # the data ready for use by your module # Training losses = [] loss_function = nn.NLLLoss() model = SkipGram(vocab_size, EMBEDDING_DIM, HIDDEN_DIM) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = 0 for input, output in data: input_index = torch.tensor([word_to_ix[input]], dtype=torch.long) model.zero_grad() log_probs = model(input_index) loss = loss_function(log_probs, torch.tensor([word_to_ix[output]], dtype=torch.long)) loss.backward() optimizer.step() total_loss += loss.item() losses.append(total_loss) print(losses) print(model.embeddings.weight[word_to_ix[&#39;spirits&#39;]]) . [912.227198600769, 903.7498636245728, 895.8144631385803, 888.3975474834442, 881.4723126888275, 875.0154674053192, 869.0011675357819, 863.3996088504791, 858.1725707054138, 853.288257598877] tensor([ 0.3021, 0.2816, -1.1773, 1.0418, 1.8390, -0.5845, -0.2637, 1.3842, 0.3855, 0.1923], grad_fn=&lt;SelectBackward&gt;) . Word2Vec (Gensim) . Gensim implements CBOW and using negative sampling for training by default. To toggle between CBOW and skip-gram algorithm, add this argument below when create the Word2Vec instance. sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW. . %%capture !pip install --upgrade gensim . from gensim.test.utils import datapath from gensim import utils class MyCorpus: &quot;&quot;&quot;An iterator that yields sentences&quot;&quot;&quot; def __iter__(self): corpus_path = datapath(&#39;lee_background.cor&#39;) for line in open(corpus_path): # assume there is one document per line, tokens separated by whitespace yield utils.simple_preprocess(line) . import gensim sentences = MyCorpus() model = gensim.models.Word2Vec(sentences=sentences, sg=0) # change to 1 if prefer skip-gram . model.wv[&#39;king&#39;] . array([-1.47548895e-02, 4.44000289e-02, 1.02321925e-02, 1.20065575e-02, 9.83571820e-03, -8.47978592e-02, 3.42624560e-02, 8.44758376e-02, -3.13533121e-03, -1.38494289e-02, -4.28904686e-03, -5.30756600e-02, 7.55382003e-03, 2.79652104e-02, 4.44820989e-03, 1.32240532e-02, -2.42202985e-03, -2.49751448e-03, -1.71462744e-02, -6.11230545e-02, 3.83632220e-02, 9.09661502e-03, 1.09449634e-02, -2.17360468e-03, -1.88374687e-02, 2.02645455e-02, -1.86126940e-02, -1.27745485e-02, -2.71721575e-02, 1.31690372e-02, 3.29722501e-02, -4.22514454e-02, 3.72793637e-02, -3.36719528e-02, -7.06554204e-03, 4.73929197e-02, 1.39981424e-02, 7.61039788e-03, -1.61971990e-02, -3.04519087e-02, -1.60803776e-02, 4.38297074e-03, -8.02283920e-03, 1.50885303e-02, 2.63876691e-02, -1.95540637e-02, -2.64777783e-02, -3.67977191e-04, 7.01137306e-03, 3.12562287e-02, 1.64159592e-02, -2.16274485e-02, -1.62629951e-02, 8.53445439e-04, -1.33869080e-02, 1.73475724e-02, -1.21692673e-03, 2.21166899e-03, -2.24457402e-02, 4.26836731e-03, -1.45576373e-02, 6.20996347e-04, 6.98805647e-03, -4.57839714e-03, -2.95367688e-02, 6.10822700e-02, 1.47746662e-02, 3.35532837e-02, -3.87191810e-02, 4.92215976e-02, -1.04450071e-02, 2.97265081e-03, 5.04135974e-02, -8.13318323e-03, 3.63118313e-02, 2.79957112e-02, -1.12850778e-03, -2.14369707e-02, -4.13609855e-02, -1.58206820e-02, -3.22486572e-02, 7.98239373e-03, -3.16767953e-02, 4.03956585e-02, -3.79999110e-05, -1.51074128e-02, 2.10159868e-02, 3.33536156e-02, 4.75050472e-02, 1.45110274e-02, 3.53002362e-02, 5.23244813e-02, 4.45592292e-02, 9.90339927e-03, 8.80143940e-02, 2.01153327e-02, 4.54641357e-02, -4.78953496e-03, 7.65400566e-03, -5.82322525e-03], dtype=float32) . Word2Vec is unsupervised task, so there is no good way to evaluate the result. Evaluation depends on the application. . Example of Gensim Word2Vec functions . import gensim.downloader as api wv = api.load(&#39;word2vec-google-news-300&#39;) . [=================================================-] 99.8% 1660.2/1662.8MB downloaded . for index , word in enumerate(wv.index_to_key): if index == 10: break print(f&quot;word #{index}/{len(wv.index_to_key)} is {word}&quot;) . word #0/3000000 is &lt;/s&gt; word #1/3000000 is in word #2/3000000 is for word #3/3000000 is that word #4/3000000 is is word #5/3000000 is on word #6/3000000 is ## word #7/3000000 is The word #8/3000000 is with word #9/3000000 is said . vec_queen = wv[&#39;queen&#39;] . One limitation of Word2Vec is that the model is unable to infer vectors for unseen words. . Note: FastText model can solve this limitation. . try: vec_random = wv[&#39;vietname&#39;] except KeyError: print(&quot;The word &#39;vietname&#39; does not appear in this model&quot;) . The word &#39;vietname&#39; does not appear in this model . pairs = [ (&#39;scooter&#39;, &#39;chair&#39;), (&#39;scooter&#39;, &#39;motorbike&#39;), (&#39;scooter&#39;, &#39;football&#39;) ] for w1, w2 in pairs: print(f&#39;{w1} t{w2} t{wv.similarity(w1, w2)}&#39;) . scooter chair 0.20833881199359894 scooter motorbike 0.7071131467819214 scooter football 0.07120829075574875 . print(wv.most_similar(positive=[&#39;vietnam&#39;], topn=5)) . [(&#39;ww2&#39;, 0.6164373159408569), (&#39;iraq&#39;, 0.6033741235733032), (&#39;reagan&#39;, 0.5772603154182434), (&#39;VietNam&#39;, 0.5732988119125366), (&#39;afghanistan&#39;, 0.5602078437805176)] . print(wv.doesnt_match([&#39;you&#39;, &quot;don&#39;t&quot;, &quot;know&quot;, &quot;me&quot;, &quot;son&quot;])) . son . Visualize the Word Embeddings using tSNE . Visualization can be used to notice semantic and syntactic trends in the data. . Semantic: words like cat, dog, cow have a tendency to lie close by. | Syntactic: words like run, running or cut, cutting lie close together. | . %matplotlib inline from sklearn.decomposition import IncrementalPCA # inital reduction from sklearn.manifold import TSNE # final reduction import numpy as np # array handling def reduce_dimensions(model): num_dimensions = 2 # final num dimensions (2D, 3D, etc) # extract the words &amp; their vectors, as numpy arrays vectors = np.asarray(model.wv.vectors) labels = np.asarray(model.wv.index_to_key) # fixed-width numpy strings # reduce using t-SNE tsne = TSNE(n_components=num_dimensions, random_state=0) vectors = tsne.fit_transform(vectors) x_vals = [v[0] for v in vectors] y_vals = [v[1] for v in vectors] return x_vals, y_vals, labels x_vals, y_vals, labels = reduce_dimensions(model) def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True): from plotly.offline import init_notebook_mode, iplot, plot import plotly.graph_objs as go trace = go.Scatter(x=x_vals, y=y_vals, mode=&#39;text&#39;, text=labels) data = [trace] if plot_in_notebook: init_notebook_mode(connected=True) iplot(data, filename=&#39;word-embedding-plot&#39;) else: plot(data, filename=&#39;word-embedding-plot.html&#39;) def plot_with_matplotlib(x_vals, y_vals, labels): import matplotlib.pyplot as plt import random random.seed(0) plt.figure(figsize=(12, 12)) plt.scatter(x_vals, y_vals) # # Label randomly subsampled 25 data points # indices = list(range(len(labels))) selected_indices = random.sample(indices, 25) for i in selected_indices: plt.annotate(labels[i], (x_vals[i], y_vals[i])) try: get_ipython() except Exception: plot_function = plot_with_matplotlib else: plot_function = plot_with_plotly plot_function(x_vals, y_vals, labels) . Using Pretrained Word2Vec in Pytorch . Construct the vocabulary of our own data. | Load word vectors corresponding to words in our vocabulary. | Use the our word2index to translate our text to indices. |",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/07/26/NLP_6_Word_Embeddings_(Word2Vec)_-Part_1.html",
            "relUrl": "/implementation/2021/07/26/NLP_6_Word_Embeddings_(Word2Vec)_-Part_1.html",
            "date": " • Jul 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Dependencies",
            "content": "Semantic and syntactic Dependencies . Use cases of Dependencies . Previously, used for feature engineering in systems | Now: more useful for human-facing applications | .",
            "url": "https://minhdang241.github.io/minhdg-blog/2021/07/25/Untitled.html",
            "relUrl": "/2021/07/25/Untitled.html",
            "date": " • Jul 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification",
            "content": "The full notebook is available here. . Introduction . Previous stuides have realized the importance of targets in aspect-based sentiment analysis. However, they all ignore the separate modeling for targets. In other word, they don&#39;t have a separate model to learn the target text. In this paper, the author proposed an architecture which has 2 sub-networks used to model the both the contexts and the targets. They argued that targets and contexts can be modelled separately, but learned from their interaction. . When “short” is collocated with “battery life”, the sentiment tends to be negative. But when “short” is used with “spoon” in the context “Short fat noodle spoon, relatively deep some curva”, the sentiment can be neu- tral. Then, the next problem is how to simultaneously model targets and contexts precisely. First, target and context can determine representations of each other. For example, when we see the target “picture quality”, context word “clear-cut” is naturally associated with the target. And it is vice versa - “picture quality” is first connected with “clear-cut” . Also, contexts and targets both includes many words. Different words may have different contributions to the final representation. Therefore, in this paper, the author created 2 attention mechanisms to capture the important information for both contexts and targets. . from IPython.display import Image Image(filename=&#39;architecture.png&#39;) . The model is called interactive attention network (IAN). It is based on LSTM and attention mechanism. . The text input will be firstly converted to embeddings. Then they are feeded into LSTMs. After that, the authors averaged the hidden states of the context LSTM to get the inital representation of context (pool vector in the figure). They do the same for the target LSTM. Then they used the target pool vector in the context attention computation and vice versa. They argued that with this design, the target and context can influence the generation of their representations interactively. Lastly, they concatenate the target and context vector and feed it into the softmax layer to do the classification. . Explain using Query, Key, Value: Regarding the target LSTM, in the figure 1 the pool vector is computed by averaging hidden states of the LSTM. That vector will play as the query vector. Each hidden state vectors represents both the key and value vectors. Then just calculate the attention score as the step below: . Step 1: Calculate the similarity score between the query vector and all the key vectors. | Step 2: Normalize the score using softmax. | Step 3: Calculate the final representation vector by weighted average the value vectors using the normalized scores. | . Install and Import required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics . import os import pickle from collections import Counter, OrderedDict from pathlib import Path from typing import Any, Dict, List, Optional, Tuple, Union from urllib.request import urlretrieve import numpy as np from tqdm import tqdm import pytorch_lightning as pl import torch import torch.nn as nn import torch.nn.functional as F import torchmetrics import torchtext from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence, pad_sequence) from torch.utils.data import DataLoader, Dataset, random_split from torchtext.data import get_tokenizer from torchtext.vocab import Vectors, Vocab # For repoducibility pl.utilities.seed.seed_everything(seed=2401, workers=True) . Global seed set to 2401 . 2401 . Define dataset, dataloader class and utility functions . class TqdmUpTo(tqdm): &quot;&quot;&quot;From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py&quot;&quot;&quot; def update_to(self, blocks=1, bsize=1, tsize=None): &quot;&quot;&quot; Parameters - blocks: int, optional Number of blocks transferred so far [default: 1]. bsize: int, optional Size of each block (in tqdm units) [default: 1]. tsize: int, optional Total size (in tqdm units). If [default: None] remains unchanged. &quot;&quot;&quot; if tsize is not None: self.total = tsize self.update(blocks * bsize - self.n) class Tokenizer(): def __init__(self, tokenizer: Any, is_lower=True): self.counter = Counter([&#39;&lt;pad&gt;&#39;, &#39;&lt;unk&gt;&#39;]) self.tokenizer = tokenizer self.vocab = self.update_vocab() self.is_lower = is_lower def update_vocab(self): sorted_by_freq_tuples = sorted(self.counter.items(), key=lambda x: x[1], reverse=True) ordered_dict = OrderedDict(sorted_by_freq_tuples) self.vocab = torchtext.vocab.vocab(ordered_dict, min_freq=1) def fit_on_texts(self, texts: List[str]): &quot;&quot;&quot; Updates internal vocabulary based on a list of texts. &quot;&quot;&quot; # lower and tokenize texts to sequences for text in texts: self.counter.update(self.tokenizer(text)) self.update_vocab() def texts_to_sequences(self, texts: List[str], reverse: bool=False, tensor: bool=True) -&gt; List[List[int]]: word2idx = self.vocab.get_stoi() sequences = [] for text in texts: if self.is_lower: text = text.lower() seq = [word2idx.get(word, word2idx[&#39;&lt;unk&gt;&#39;]) for word in self.tokenizer(text)] if reverse: seq = seq[::-1] if tensor: seq = torch.tensor(seq) sequences.append(seq) return sequences def text_to_sequence(self, text: str, reverse: bool=False, tensor: bool=True) -&gt; List[int]: if self.is_lower: text = text.lower() word2idx = self.vocab.get_stoi() seq = [word2idx.get(word, word2idx[&#39;&lt;unk&gt;&#39;]) for word in self.tokenizer(text)] if reverse: seq = seq[::-1] if tensor: seq = torch.tensor(seq) return seq def download_url(url, filename, directory=&#39;.&#39;): &quot;&quot;&quot;Download a file from url to filename, with a progress bar.&quot;&quot;&quot; if not os.path.exists(directory): os.makedirs(directory) path = os.path.join(directory, filename) with TqdmUpTo(unit=&quot;B&quot;, unit_scale=True, unit_divisor=1024, miniters=1) as t: urlretrieve(url, path, reporthook=t.update_to, data=None) # nosec return path def load_data_from(path: Union[str, Path]): sentences = [] targets = [] sentiments = [] with open(path, &#39;r&#39;) as f: lines = f.readlines() for index in range(0, len(lines), 3): text = lines[index].lower().strip() target = lines[index+1].lower().strip() text = text.replace(&#39;$t$&#39;, target) sentences.append(text) targets.append(target) sentiments.append(int(lines[index+2].strip())) return sentences, targets, sentiments def _preprocess_data(data, tokenizer): sentences, targets, sentiments = data # Create sentence sequences and aspects sequences sequences = tokenizer.texts_to_sequences(sentences) target_seqs = tokenizer.texts_to_sequences(targets) sentiments = torch.tensor(sentiments) + 1 # pad sequences seq_lens = torch.tensor([len(seq) for seq in sequences]) target_lens = torch.tensor([len(target_seq) for target_seq in target_seqs]) sequences = pad_sequence(sequences, batch_first=True) target_seqs = pad_sequence(target_seqs, batch_first=True) assert len(sequences) == len(sentiments) assert len(sequences) == len(target_seqs) all_data = [] for i in range(len(sentiments)): sample = { &#39;context_seq&#39;: sequences[i], &#39;context_len&#39;: seq_lens[i], &#39;target_seq&#39;: target_seqs[i], &#39;target_len&#39;: target_lens[i], &#39;sentiment&#39;: sentiments[i] } all_data.append(sample) return all_data def build_vocab(tokenizer, data): sentences = data[0] tokenizer.fit_on_texts(sentences) def load_pretrained_word_embeddings(options: Dict[str, Any]): return torchtext.vocab.GloVe(options[&#39;name&#39;], options[&#39;dim&#39;]) def create_embedding_matrix(word_embeddings: Vectors, vocab: Vocab, path: Union[str, Path]): if os.path.exists(path): print(f&#39;loading embedding matrix from {path}&#39;) embedding_matrix = pickle.load(open(path, &#39;rb&#39;)) else: embedding_matrix = torch.zeros((len(vocab), word_embeddings.dim), dtype=torch.float) # words that are not availabel in the pretrained word embeddings will be zeros for word, index in vocab.get_stoi().items(): embedding_matrix[index] = word_embeddings.get_vecs_by_tokens(word) # save embedding matrix pickle.dump(embedding_matrix, open(path, &#39;wb&#39;)) return embedding_matrix . class SemEvalDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] . RES_TRAIN_DS_URL = &#39;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/semeval14/Restaurants_Train.xml.seg&#39; RES_TEST_DS_URL = &#39;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/semeval14/Restaurants_Test_Gold.xml.seg&#39; # Laptop LAP_TRAIN_DS_URL = &#39;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/semeval14/Laptops_Train.xml.seg&#39; LAP_TEST_DS_URL = &#39;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/semeval14/Laptops_Test_Gold.xml.seg&#39; class SemEval2014(pl.LightningDataModule): def __init__(self, tokenizer, opts): super().__init__() self.tokenizer = tokenizer self.batch_size = opts[&#39;batch_size&#39;] self.num_workers = opts[&#39;num_workers&#39;] self.on_gpu = opts[&#39;on_gpu&#39;] self.mapping = {&quot;negative&quot;: 0, &quot;neutral&quot;: 1, &quot;positive&quot;: 2} self.inverse_mapping = {v: k for k, v in enumerate(self.mapping)} def prepare_data(self) -&gt; None: self.train_path = &#39;download/SemEval2014/train.xml&#39; self.test_path = &#39;download/SemEval2014/test.xml&#39; if not os.path.exists(train_path): print(&quot;Downloading train dataset&quot;) self.train_path = download_url(RES_TRAIN_DS_URL, &#39;train.xml&#39;, &#39;download/SemEval2014&#39;) if not os.path.exists(test_path): print(&quot;Downloading test dataset&quot;) self.test_path = download_url(RES_TEST_DS_URL, &#39;test.xml&#39;, &#39;download/SemEval2014&#39;) def setup(self, stage: str = None) -&gt; None: if stage == &#39;fit&#39; or stage is None: # Load data from files train_data = load_data_from(self.train_path) valid_data = load_data_from(self.test_path) self.train_data = _preprocess_data(train_data, self.tokenizer) self.val_data = _preprocess_data(valid_data, self.tokenizer) elif stage == &#39;test&#39; or stage is None: test_data = load_data_from(self.test_path) self.test_data = _preprocess_data(test_data, self.tokenizer) def train_dataloader(self): # Create Dataset object train_ds = SemEvalDataset(self.train_data) # Create Dataloader return DataLoader( train_ds, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def val_dataloader(self): val_ds = SemEvalDataset(self.val_data) return DataLoader( val_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def test_dataloader(self): test_ds = SemEvalDataset(self.test_data) return DataLoader( test_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def __repr__(self): basic = f&quot;SemEval2014 Dataset nNum classes: {len(self.mapping)} nMapping: {self.mapping} n&quot; if self.train_data is None and self.val_data is None and self.test_data is None: return basic batch = next(iter(self.train_dataloader())) cols = [&#39;context_seq&#39;, &#39;context_len&#39;, &#39;target_seq&#39;, &#39;target_len&#39;, &#39;sentiment&#39;] context_seqs, context_lens, target_seqs, target_lens, sentiments = [batch[col] for col in cols] data = ( f&quot;Train/val/test sizes: {len(self.train_data)}, {len(self.val_data)}, {len(self.test_data)} n&quot; f&quot;Batch context_seqs stats: {(context_seqs.shape, context_seqs.dtype)} n&quot; f&quot;Batch context_lens stats: {(context_lens.shape, context_lens.dtype)} n&quot; f&quot;Batch target_seqs stats: {(target_seqs.shape, target_seqs.dtype)} n&quot; f&quot;Batch target_lens stats: {(target_lens.shape, target_lens.dtype)} n&quot; f&quot;Batch sentiments stats: {(sentiments.shape, sentiments.dtype)} n&quot; ) return basic + data . Implementation . class Attention(nn.Module): def __init__(self, hidden_size): super(Attention, self).__init__() self.linear = nn.Linear(hidden_size, hidden_size) def forward(self, query, key, value, max_seq_len, seq_lens, device): # Calculate similarity between query and key vectors score = torch.tanh(torch.matmul( self.linear(key), query.transpose(-2,-1))).squeeze(-1) # BxL # Mask out padding score att_mask = torch.arange(max_seq_len, device=device)[None,:] &lt; seq_lens[:, None] score = score.masked_fill(att_mask == False, float(&#39;-inf&#39;)) softmax_score = F.softmax(score, dim=-1).unsqueeze(2) #BxLx1 out = torch.matmul(value.transpose(-2,-1), softmax_score).squeeze() #BxH return out . class IAN(pl.LightningModule): def __init__(self, embedding_matrix, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.0): super().__init__() embedding_dim = embedding_matrix.shape[1] self.batch_first = batch_first self.lr = lr self.l2reg = l2reg # Define architecture components self.embedding = nn.Embedding.from_pretrained(embedding_matrix) self.target_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.context_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.context_attn = Attention(hidden_size) self.target_attn = Attention(hidden_size) self.linear = nn.Linear(hidden_size*2, num_classes) # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.test_acc = torchmetrics.Accuracy() # Initialize layer parameters # for layer in [self.context_lstm, self.target_lstm, # self.context_attention, self.target_attention, self.linear]: # nn.init.uniform_(layer.weight, a=-0.1, b=0.1) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, input): cols = [&quot;context_seq&quot;, &quot;context_len&quot;, &quot;target_seq&quot;, &quot;target_len&quot;] padded_context_seqs, context_lens, padded_target_seqs, target_lens = [input[col] for col in cols] padded_context_embeddings = self.embedding(padded_context_seqs) padded_target_embeddings = self.embedding(padded_target_seqs) context_seqs_pack = pack_padded_sequence(padded_context_embeddings, context_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False) target_seqs_pack = pack_padded_sequence(padded_target_embeddings, target_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False) H_context, _ = self.context_lstm(context_seqs_pack) H_target, _ = self.target_lstm(target_seqs_pack) # Unpack to get the full hidden states padded_H_context, _ = pad_packed_sequence(H_context, batch_first=self.batch_first) # BxLxH padded_H_target, _ = pad_packed_sequence(H_target, batch_first=self.batch_first) # BxLxH # Compute the initial representation for target and context c_avg = torch.mean(padded_H_context, dim=1, keepdim=True) #Bx1xH t_avg = torch.mean(padded_H_target, dim=1, keepdim=True) #Bx1xH c_max_seq_len = torch.max(context_lens) final_c = self.context_attn(t_avg, padded_H_context, padded_H_context, c_max_seq_len, context_lens, self.device) t_max_seq_len = torch.max(target_lens) inal_t = self.target_attn(c_avg, padded_H_target, padded_H_target, t_max_seq_len, target_lens, self.device) final_vector = torch.cat([final_t, final_c], dim=-1) # Bx2H out = self.linear(final_vector) logits = torch.tanh(out) return logits def training_step(self, batch, batch_idx): sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) . Training . processed_train_data = _preprocess_data(train_data, tokenizer) . train_path = download_url(RES_TRAIN_DS_URL, &#39;train.xml&#39;, &#39;download/SemEval2014&#39;) test_path = download_url(RES_TEST_DS_URL, &#39;test.xml&#39;, &#39;download/SemEval2014&#39;) train_data = load_data_from(train_path) test_data = load_data_from(test_path) all_sentences = train_data[0] + test_data[0] tokenizer = Tokenizer(get_tokenizer(&quot;basic_english&quot;)) build_vocab(tokenizer, [all_sentences]) . 384kB [00:00, 1.71MB/s] 120kB [00:00, 626kB/s] . word_embeddings = load_pretrained_word_embeddings({&quot;name&quot;: &quot;42B&quot;, &quot;dim&quot;: 300}) . .vector_cache/glove.42B.300d.zip: 1.88GB [05:53, 5.31MB/s] 100%|█████████▉| 1916797/1917494 [04:10&lt;00:00, 8178.84it/s] . train_path = download_url(RES_TRAIN_DS_URL, &#39;train.xml&#39;, &#39;download/SemEval2014&#39;) test_path = download_url(RES_TEST_DS_URL, &#39;test.xml&#39;, &#39;download/SemEval2014&#39;) train_data = load_data_from(train_path) test_data = load_data_from(test_path) all_sentences = train_data[0] + test_data[0] tokenizer = Tokenizer(get_tokenizer(&quot;basic_english&quot;)) build_vocab(tokenizer, [all_sentences]) options = { &quot;on_gpu&quot;: True, &quot;batch_size&quot;: 16, &quot;num_workers&quot;: 2 } datamodule = SemEval2014(tokenizer, options) embedding_matrix = create_embedding_matrix(word_embeddings, tokenizer.vocab, &quot;embedding_matrix.dat&quot;) . 384kB [00:00, 7.09MB/s] 120kB [00:00, 2.81MB/s] . loading embedding matrix from embedding_matrix.dat . torch.autograd.set_detect_anomaly(True) checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 aspect_embedding_dim = 300 num_epochs = 30 l2reg = 1e-5 dropout = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback], deterministic=True) # trainer = pl.Trainer(fast_dev_run=True, gpus=1) #Debug # trainer = pl.Trainer(overfit_batches=0.025, max_epochs=num_epochs, gpus=1) #Debug model = IAN(embedding_matrix=embedding_matrix, hidden_size=hidden_size, lr=lr, l2reg=l2reg, dropout=dropout) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 1.4 M 1 | target_lstm | LSTM | 722 K 2 | context_lstm | LSTM | 722 K 3 | context_attn | Attention | 90.3 K 4 | target_attn | Attention | 90.3 K 5 | linear | Linear | 1.8 K 6 | train_acc | Accuracy | 0 7 | val_acc | Accuracy | 0 8 | test_acc | Accuracy | 0 - 1.6 M Trainable params 1.4 M Non-trainable params 3.0 M Total params 11.921 Total estimated model params size (MB) Global seed set to 2401 . . trainer.test(ckpt_path=checkpoint_callback.best_model_path, test_dataloaders=datamodule.test_dataloader()) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.7866071462631226} -- . [{&#39;test_acc&#39;: 0.7866071462631226}] . Discussion . Our result: . Dataset Restaurants . IAN | 0.786 | . Paper result: . Dataset Restaurants Laptops . No-target | 0.772 | 0.708 | . No-interaction | 0.769 | 0.706 | . Target2Content | 0.775 | 0.712 | . IAN | 0.786 | 0.721 | . From the two table above, we can see that our training get the same accuracy value with the paper. . Analysis: . The author analyzed the IAN model effectiveness by comparing it with 3 other types of models. All the models are based on LSTM and attention mechanism. The No-target model does not model the representation of the target. The second model, No-interaction, used 2 LSTM networks to model the representations of target and context via their own local atttentions, but without interaction. Next, the Target2Content model also employs 2 LSTM networks to learn target and context representation, but only uses the pool target vector for the context attention computation. The difference between this model and the IAN is the IAN also use the pool context vector in target attetion computation. . The results verify that target should be separately modeled and target representations can make contribution to judging the sentiment polarity of a target. . The improvements on Restaurant category is much less than those on Laptop category. The author explained that by pointing out that the Restaurant dataset has 9% 1-word target more than the Laptop one. In other words, the Laptop dataset has more multi-words targets. In IAN, the targets are modeled by LSTM networks and interactive attentions. LSTM networks and interactive attention are more effective on modelling long targets than short ones. . You can read more about the case study in which the author analyzes the attention score when doing inference here. . Lesson . Pass device in forward function instead of __init__ | When masking, create a mask matrix and times with the matrix we want to mask. By doing that, we can avoid modifying the tensor in-place error. We can also use the function mask_fill with &#39;underscore&#39;. | When the training model is slow, check the number of model parameters! | When using functions that requires the dim, we should set it explicitly to avoid bugs in our code. For example, in this implementation, using squeeze() function after calculating the similariry score between query and key vectors has error when the sequence length of key is 1. | Suggestion for Readers: . You can try a larger word embeddings to see whether we can improve the metrics. | Training on the Laptop data | Have fun :) |",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/07/04/NLP_5_Interactive_Attention_Networks_for_Aspect_Level_Sentiment_Classififcation.html",
            "relUrl": "/implementation/2021/07/04/NLP_5_Interactive_Attention_Networks_for_Aspect_Level_Sentiment_Classififcation.html",
            "date": " • Jul 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Attention based LSTM for Aspect level Sentiment Classification",
            "content": "The full notebook is available here. . Introduction . Aspect-level sentiment classification is a fine- grained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, “The appetizers are ok, but the service is slow.”, for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-of- the-art performance on aspect-level sentiment classification. . Install required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics . Import required packages . import os import pickle from collections import Counter, OrderedDict from pathlib import Path from typing import Any, Dict, List, Optional, Tuple, Union from urllib.request import urlretrieve import numpy as np from tqdm import tqdm import pytorch_lightning as pl import torch import torch.nn as nn import torch.nn.functional as F import torchmetrics import torchtext from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence, pad_sequence) from torch.utils.data import DataLoader, Dataset, random_split from torchtext.data import get_tokenizer from torchtext.vocab import Vectors, Vocab import xml.etree.ElementTree as ET # For repoducibility pl.utilities.seed.seed_everything(seed=2401, workers=True) . /usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/__init__.py:44: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5 &#34;`pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package&#34; Global seed set to 2401 . 2401 . Define dataset, data module class, utils function . class TqdmUpTo(tqdm): &quot;&quot;&quot;From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py&quot;&quot;&quot; def update_to(self, blocks=1, bsize=1, tsize=None): &quot;&quot;&quot; Parameters - blocks: int, optional Number of blocks transferred so far [default: 1]. bsize: int, optional Size of each block (in tqdm units) [default: 1]. tsize: int, optional Total size (in tqdm units). If [default: None] remains unchanged. &quot;&quot;&quot; if tsize is not None: self.total = tsize self.update(blocks * bsize - self.n) class Tokenizer(): def __init__(self, tokenizer: Any): self.counter = Counter([&#39;&lt;pad&gt;&#39;, &#39;&lt;unk&gt;&#39;]) self.tokenizer = tokenizer self.vocab = self.update_vocab() def update_vocab(self): sorted_by_freq_tuples = sorted(self.counter.items(), key=lambda x: x[1], reverse=True) ordered_dict = OrderedDict(sorted_by_freq_tuples) self.vocab = torchtext.vocab.vocab(ordered_dict, min_freq=1) def fit_on_texts(self, texts: List[str]): &quot;&quot;&quot; Updates internal vocabulary based on a list of texts. &quot;&quot;&quot; # lower and tokenize texts to sequences for text in texts: self.counter.update(self.tokenizer(text)) self.update_vocab() def texts_to_sequences(self, texts: List[str], reverse: bool=False, tensor: bool=True) -&gt; List[int]: word2idx = self.vocab.get_stoi() sequences = [] for text in texts: seq = [word2idx.get(word, word2idx[&#39;&lt;unk&gt;&#39;]) for word in self.tokenizer(text)] if reverse: seq = seq[::-1] if tensor: seq = torch.tensor(seq) sequences.append(seq) return sequences def download_url(url, filename, directory=&#39;.&#39;): &quot;&quot;&quot;Download a file from url to filename, with a progress bar.&quot;&quot;&quot; if not os.path.exists(directory): os.makedirs(directory) path = os.path.join(directory, filename) with TqdmUpTo(unit=&quot;B&quot;, unit_scale=True, unit_divisor=1024, miniters=1) as t: urlretrieve(url, path, reporthook=t.update_to, data=None) # nosec return path def _preprocess_data(data, tokenizer, mapping_polarity, mapping_aspect): sentences, text_aspects, text_sentiments = data # Create sentence sequences and aspects sequences sequences = tokenizer.texts_to_sequences(sentences) sentiments = [] for val in text_sentiments: sentiments.append(mapping_polarity[val]) sentiments = torch.tensor(sentiments) aspects = [] for val in text_aspects: aspects.append(mapping_aspect[val]) aspects = torch.tensor(aspects) # pad sequences seq_lens = torch.tensor([len(seq) for seq in sequences]) sequences = pad_sequence(sequences, batch_first=True) assert len(sequences) == len(sentiments) assert len(sequences) == len(aspects) all_data = [] for i in range(len(sentiments)): sample = { &#39;sequence&#39;: sequences[i], &#39;seq_len&#39;: seq_lens[i], &#39;aspect&#39;: aspects[i], &#39;sentiment&#39;: sentiments[i] } all_data.append(sample) return all_data def load_data_from(path: Union[str, Path]): tree = ET.parse(path) root = tree.getroot() sentences = [] aspects = [] sentiments = [] for sent in root: text = &#39;&#39; for i, child in enumerate(sent): if i == 0: text = child.text elif i == 2: for aspect in child: # Get polarities polarity = aspect.attrib[&#39;polarity&#39;].lower().strip() if polarity == &quot;conflict&quot;: continue sentiments.append(polarity) # Get aspects asp = aspect.attrib[&#39;category&#39;].lower().strip() if asp == &#39;anecdotes/miscellaneous&#39;: aspects.append(&#39;miscellaneous&#39;) else: aspects.append(asp) # Get sentences sentences.append(text) return sentences, aspects, sentiments def build_vocab(tokenizer, data): sentences = data[0] tokenizer.fit_on_texts(sentences) def load_pretrained_word_embeddings(options: Dict[str, Any]): return torchtext.vocab.GloVe(options[&#39;name&#39;], options[&#39;dim&#39;]) def create_embedding_matrix(word_embeddings: Vectors, vocab: Vocab, path: Union[str, Path]): if os.path.exists(path): print(f&#39;loading embedding matrix from {path}&#39;) embedding_matrix = pickle.load(open(path, &#39;rb&#39;)) else: embedding_matrix = torch.zeros((len(vocab), word_embeddings.dim), dtype=torch.float) # words that are not availabel in the pretrained word embeddings will be zeros for word, index in vocab.get_stoi().items(): embedding_matrix[index] = word_embeddings.get_vecs_by_tokens(word) # save embedding matrix pickle.dump(embedding_matrix, open(path, &#39;wb&#39;)) return embedding_matrix . class SemEvalDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] . TRAIN_DS_URL = &#39;https://raw.githubusercontent.com/zhangjf-nlp/ATAE-LSTM/master/data/restaurants-train.xml&#39; VALID_DS_URL = &#39;https://raw.githubusercontent.com/zhangjf-nlp/ATAE-LSTM/master/data/restaurants-trial.xml&#39; TEST_DS_URL = &quot;https://raw.githubusercontent.com/AlexYangLi/ABSA_Keras/master/raw_data/semeval14_restaurant/Restaurants_Test_Gold.xml&quot; class SemEval2014(pl.LightningDataModule): def __init__(self, tokenizer, opts): super().__init__() self.tokenizer = tokenizer self.batch_size = opts[&#39;batch_size&#39;] self.num_workers = opts[&#39;num_workers&#39;] self.on_gpu = opts[&#39;on_gpu&#39;] self.mapping = {&quot;negative&quot;: 0, &quot;neutral&quot;: 1, &quot;positive&quot;: 2} self.mapping_aspects = {&#39;ambience&#39;: 0, &#39;food&#39;: 1, &#39;miscellaneous&#39;: 2, &#39;price&#39;: 3, &#39;service&#39;: 4} self.inverse_mapping = {v: k for k, v in enumerate(self.mapping)} def prepare_data(self, *args, **kwargs) -&gt; None: self.train_path = &#39;download/SemEval2014/train.xml&#39; self.valid_path = &#39;download/SemEval2014/valid.xml&#39; self.test_path = &#39;download/SemEval2014/test.xml&#39; if not os.path.exists(train_path): print(&quot;Downloading train dataset&quot;) self.train_path = download_url(TRAIN_DS_URL, &#39;train.xml&#39;, &#39;download/SemEval2014&#39;) if not os.path.exists(valid_path): print(&quot;Downloading valid dataset&quot;) self.valid_path = download_url(VALID_DS_URL, &#39;valid.xml&#39;, &#39;download/SemEval2014&#39;) if not os.path.exists(test_path): print(&quot;Downloading test dataset&quot;) self.test_path = download_url(TEST_DS_URL, &#39;test.xml&#39;, &#39;download/SemEval2014&#39;) def setup(self, stage: str = None) -&gt; None: if stage == &#39;fit&#39; or stage is None: # Load data from files train_data = load_data_from(self.train_path) valid_data = load_data_from(self.valid_path) self.train_data = _preprocess_data(train_data, self.tokenizer, self.mapping, self.mapping_aspects) self.val_data = _preprocess_data(valid_data, self.tokenizer, self.mapping, self.mapping_aspects) elif stage == &#39;test&#39; or stage is None: test_data = load_data_from(self.test_path) self.test_data = _preprocess_data(test_data, self.tokenizer, self.mapping, self.mapping_aspects) def train_dataloader(self): # Create Dataset object train_ds = SemEvalDataset(self.train_data) # Create Dataloader return DataLoader( train_ds, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def val_dataloader(self): val_ds = SemEvalDataset(self.val_data) return DataLoader( val_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def test_dataloader(self): test_ds = SemEvalDataset(self.test_data) return DataLoader( test_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def __repr__(self): basic = f&quot;SemEval2014 Dataset nNum classes: {len(self.mapping)} nMapping: {self.mapping} n&quot; if self.train_data is None and self.val_data is None and self.test_data is None: return basic batch = next(iter(self.train_dataloader())) sequences, seq_lens, aspects, sentiments = batch[&#39;sequence&#39;], batch[&#39;seq_len&#39;], batch[&#39;aspect&#39;], batch[&#39;sentiment&#39;] data = ( f&quot;Train/val/test sizes: {len(self.train_data)}, {len(self.val_data)}, {len(self.test_data)} n&quot; f&quot;Batch sequences stats: {(sequences.shape, sequences.dtype)} n&quot; f&quot;Batch seq_lens stats: {(seq_lens.shape, seq_lens.dtype)} n&quot; f&quot;Batch aspects stats: {(aspects.shape, aspects.dtype)} n&quot; f&quot;Batch sentiments stats: {(sentiments.shape, sentiments.dtype)} n&quot; ) return basic + data . Implementation . AT-LSTM . LSTM with Aspect Embedding (AE-LSTM) . Aspect information is important when doing classificaiton on the sentence. We may get different polarities with different aspects. The author proposed to learn an embedding vector for each aspect. . Attention-based LSTM (AT-LSTM) . The standard LSTM cannot detect which is the important part for aspect-level sentiment classification. The author proposed to design an attention mechanism capturing the key part of sentence in response to a given aspect. . from IPython.display import Image Image(filename=&#39;/Users/minhdang/Desktop/AT-LSTM.png&#39;) . class AT_LSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, aspect_hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.aspect_embedding = nn.Embedding(5, embedding_dim) self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear_h = nn.Linear(hidden_size, hidden_size, bias=False) self.linear_v = nn.Linear(aspect_hidden_size, aspect_hidden_size, bias=False) self.linear_p = nn.Linear(hidden_size, hidden_size, bias=False) self.linear_x = nn.Linear(hidden_size, hidden_size, bias=False) self.linear = nn.Linear(hidden_size + aspect_hidden_size, 1) self.linear_s = nn.Linear(hidden_size, num_classes) self.batch_first = batch_first self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, input): sequences, seq_lens, aspect_seqs = input[&#39;sequence&#39;], input[&#39;seq_len&#39;], input[&#39;aspect&#39;] # Covert sequence to embeddings embeds = self.embedding(sequences) # Get the max sequence length max_seq_len = torch.max(seq_lens) # Convert aspect to embeddings aspect_embeds = self.aspect_embedding(aspect_seqs) packed_embeds = pack_padded_sequence(embeds, seq_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False) H, (h, c) = self.lstm(packed_embeds) padded_H, lens = pad_packed_sequence(H, batch_first=True) Wh_H = self.linear_h(padded_H) Wv_va = self.linear_v(aspect_embeds) Wv_va = Wv_va.unsqueeze(1).repeat(1, max_seq_len, 1) M = torch.tanh(torch.cat([Wh_H, Wv_va], dim=-1)) # Calculate attention score score = self.linear(M).squeeze() att_mask = torch.arange(max_seq_len, device=self.device)[None,:] &lt; seq_lens[:, None] # Create mask to zero out attention scores for padding tokens score[~att_mask] = float(&#39;-inf&#39;) alpha = F.softmax(score, dim=-1).unsqueeze(2) r = torch.matmul(padded_H.transpose(-2,-1), alpha).squeeze() final_h = torch.tanh(self.linear_p(r) + self.linear_x(h[-1])) out = self.linear_s(final_h) return out def training_step(self, batch, batch_idx): sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . ATAE-LSTM . To take advanatage of aspect information, we append the input aspect embedding into each word input vector. By doing that, the inter-dependence between words and the input aspect can be modeled. . from IPython.display import Image Image(filename=&#39;/Users/minhdang/Desktop/ATAE-LSTM.png&#39;) . class ATAE_LSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, aspect_hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.aspect_embedding = nn.Embedding(5, embedding_dim) self.lstm = nn.LSTM(2*embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear_h = nn.Linear(hidden_size, hidden_size, bias=False) self.linear_v = nn.Linear(aspect_hidden_size, aspect_hidden_size, bias=False) self.linear_p = nn.Linear(hidden_size, hidden_size, bias=False) self.linear_x = nn.Linear(hidden_size, hidden_size, bias=False) self.linear = nn.Linear(hidden_size + aspect_hidden_size, 1) self.linear_s = nn.Linear(hidden_size, num_classes) self.batch_first = batch_first self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, input): sequences, seq_lens, aspect_seqs = input[&#39;sequence&#39;], input[&#39;seq_len&#39;], input[&#39;aspect&#39;] # Covert sequence to embeddings embeds = self.embedding(sequences) # Get the max sequence length max_seq_len = torch.max(seq_lens) # Convert aspect to embeddings aspect_embeds = self.aspect_embedding(aspect_seqs) # Repeat the aspect vector across the dimension 1 aspect_embeds_repeat = aspect_embeds.unsqueeze(1).repeat(1,embeds.shape[1],1) # Append the aspect vector to the input word vector embeds = torch.cat([embeds, aspect_embeds_repeat], dim=-1) packed_embeds = pack_padded_sequence(embeds, seq_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False) H, (h, c) = self.lstm(packed_embeds) padded_H, lens = pad_packed_sequence(H, batch_first=True) Wh_H = self.linear_h(padded_H) Wv_va = self.linear_v(aspect_embeds) Wv_va = Wv_va.unsqueeze(1).repeat(1, max_seq_len, 1) M = torch.tanh(torch.cat([Wh_H, Wv_va], dim=-1)) # Calculate attention score score = self.linear(M).squeeze() # Create mask to zero out attention scores for padding tokens att_mask = torch.arange(max_seq_len, device=self.device)[None,:] &lt; seq_lens[:, None] score[~att_mask] = float(&#39;-inf&#39;) alpha = F.softmax(score, dim=-1).unsqueeze(2) r = torch.matmul(padded_H.transpose(-2,-1), alpha).squeeze() final_h = torch.tanh(self.linear_p(r) + self.linear_x(h[-1])) out = self.linear_s(final_h) return out def training_step(self, batch, batch_idx): sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . Training . word_embeddings = load_pretrained_word_embeddings({&quot;name&quot;: &quot;840B&quot;, &quot;dim&quot;: 300}) . train_path = download_url(TRAIN_DS_URL, &#39;train.xml&#39;, &#39;download/SemEval2014&#39;) valid_path = download_url(VALID_DS_URL, &#39;valid.xml&#39;, &#39;download/SemEval2014&#39;) test_path = download_url(TEST_DS_URL, &#39;test.xml&#39;, &#39;download/SemEval2014&#39;) train_data = load_data_from(train_path) valid_data = load_data_from(valid_path) test_data = load_data_from(test_path) all_sentences = train_data[0] + valid_data[0] + test_data[0] tokenizer = Tokenizer(get_tokenizer(&quot;basic_english&quot;)) build_vocab(tokenizer, [all_sentences]) options = { &quot;on_gpu&quot;: True, &quot;batch_size&quot;: 64, &quot;num_workers&quot;: 2 } datamodule = SemEval2014(tokenizer, options) embedding_matrix = create_embedding_matrix(word_embeddings, tokenizer.vocab, &quot;embedding_matrix.dat&quot;) . 1.18MB [00:00, 9.47MB/s] 40.0kB [00:00, 1.78MB/s] 352kB [00:00, 12.3MB/s] . loading embedding matrix from embedding_matrix.dat . AT-LSTM . checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 aspect_embedding_dim = 300 num_epochs = 30 l2reg = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback], deterministic=True) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.025, max_epochs=num_epochs) #Debug model = AT_LSTM(embedding_matrix, hidden_size, aspect_embedding_dim, lr=lr, l2reg=l2reg) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | embedding | Embedding | 1.3 M 1 | aspect_embedding | Embedding | 1.5 K 2 | lstm | LSTM | 722 K 3 | linear_h | Linear | 90.0 K 4 | linear_v | Linear | 90.0 K 5 | linear_p | Linear | 90.0 K 6 | linear_x | Linear | 90.0 K 7 | linear | Linear | 601 8 | linear_s | Linear | 903 9 | train_acc | Accuracy | 0 10 | val_acc | Accuracy | 0 11 | val_f1 | F1 | 0 12 | test_acc | Accuracy | 0 13 | test_f1 | F1 | 0 1.1 M Trainable params 1.3 M Non-trainable params 2.4 M Total params 9.708 Total estimated model params size (MB) Global seed set to 2401 . . trainer.test(ckpt_path=checkpoint_callback.best_model_path) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.8150510191917419, &#39;test_f1&#39;: 0.6747192740440369} -- . [{&#39;test_acc&#39;: 0.8150510191917419, &#39;test_f1&#39;: 0.6747192740440369}] . ATAE-LSTM . checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 aspect_embedding_dim = 300 num_epochs = 30 l2reg = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback], deterministic=True) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.025, max_epochs=num_epochs) #Debug model = ATAE_LSTM(embedding_matrix, hidden_size, aspect_embedding_dim, lr=lr, l2reg=l2reg) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params 0 | embedding | Embedding | 1.3 M 1 | aspect_embedding | Embedding | 1.5 K 2 | lstm | LSTM | 1.1 M 3 | linear_h | Linear | 90.0 K 4 | linear_v | Linear | 90.0 K 5 | linear_p | Linear | 90.0 K 6 | linear_x | Linear | 90.0 K 7 | linear | Linear | 601 8 | linear_s | Linear | 903 9 | train_acc | Accuracy | 0 10 | val_acc | Accuracy | 0 11 | val_f1 | F1 | 0 12 | test_acc | Accuracy | 0 13 | test_f1 | F1 | 0 1.4 M Trainable params 1.3 M Non-trainable params 2.8 M Total params 11.148 Total estimated model params size (MB) Global seed set to 2401 . . trainer.test(ckpt_path=checkpoint_callback.best_model_path) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.8227040767669678, &#39;test_f1&#39;: 0.6682634353637695} -- . [{&#39;test_acc&#39;: 0.8227040767669678, &#39;test_f1&#39;: 0.6682634353637695}] . Discussion . Since I could not find out the exact dataset used in the paper, it is impossible to compare with the paper&#39;s result. Instead, I will compare my implementation with other implementations from Github. . Our result . Model Accuracy F1 macro . AT-LSTM | 0.81 | 0.674 | . ATAE-LSTM | 0.82 | 0.668 | . There is a unbalance between positive, negative and positive classes in both train, valid and test set. That&#39;s the reason why we have the F1 macro lower than the accuracy. .",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/06/26/NLP_4_Attention_based_LSTM_for_Aspect_level_Sentiment_Classification.html",
            "relUrl": "/implementation/2021/06/26/NLP_4_Attention_based_LSTM_for_Aspect_level_Sentiment_Classification.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "PhoBERT Vietnamese Sentiment Analysis on UIT-VSFC dataset with transformers and Pytorch Lightning",
            "content": "The full notebook is available here. . Introduction . PhoBERT: Pre-trained language models for Vietnamese . PhoBERT models are the SOTA language models for Vietnamese. There are two versions of PhoBERT, which are PhoBERT base and PhoBERT large. Their pretraining approach is based on RoBERTa which optimizes the BERT pre-training procedure for more robust performance. PhoBERT has achieved SOTA in many downstream task such as POS, Dependency parsing, NER and NLI. You can read more about the PhoBERT here. . UIT-VSFC: Vietnamese Students&#8217; Feedback Corpus . Vietnamese Students’ Feedback Corpus (UIT-VSFC) is the resource consists of over 16,000 sentences which are human-annotated with two different tasks: sentiment-based and topic-based classifications. . In this project, we will apply PhoBERT to do the sentiment classification task on UIT-VSFC dataset. We will use pytorch-lightning and transformers for this project. . Install required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics !pip install transformers !pip install datasets . Import required packages . import os import zipfile from pathlib import Path from typing import Any, Dict, List, Optional, Tuple, Union from urllib.request import urlretrieve import pandas as pd from tqdm import tqdm import pytorch_lightning as pl import torch import torch.nn.functional as F import torchmetrics from datasets import load_dataset from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.utils.data import DataLoader from transformers import (AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding) # For repoducibility pl.utilities.seed.seed_everything(seed=2401, workers=True) . Global seed set to 2401 . 2401 . Define dataset, dataloader class and utility functions . class TqdmUpTo(tqdm): &quot;&quot;&quot;From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py&quot;&quot;&quot; def update_to(self, blocks=1, bsize=1, tsize=None): &quot;&quot;&quot; Parameters - blocks: int, optional Number of blocks transferred so far [default: 1]. bsize: int, optional Size of each block (in tqdm units) [default: 1]. tsize: int, optional Total size (in tqdm units). If [default: None] remains unchanged. &quot;&quot;&quot; if tsize is not None: self.total = tsize # pylint: disable=attribute-defined-outside-init self.update(blocks * bsize - self.n) # will also set self.n = b * bsize def download_url(url, filename, directory=&#39;.&#39;): &quot;&quot;&quot;Download a file from url to filename, with a progress bar.&quot;&quot;&quot; if not os.path.exists(directory): os.makedirs(directory) path = os.path.join(directory, filename) with TqdmUpTo(unit=&quot;B&quot;, unit_scale=True, unit_divisor=1024, miniters=1) as t: urlretrieve(url, path, reporthook=t.update_to, data=None) # nosec return path def _load_data_from(data_dir: Union[str, Path]): fnames = [&#39;sentiments.txt&#39;, &#39;sents.txt&#39;, &#39;topics.txt&#39;] sentiments = [] sents = [] topics = [] for name in fnames: with open(f&quot;{data_dir}/{name}&quot;, &#39;r&#39;) as f: if name == &quot;sentiments.txt&quot;: sentiments = [int(line.strip()) for line in f.readlines()] elif name == &quot;sents.txt&quot;: sents = [line.strip() for line in f.readlines()] else: topics = [int(line.strip()) for line in f.readlines()] return sents, sentiments, topics def _save_to_csv(file_path: Union[str, Path], data): sents, sentiments, topics = data df = pd.DataFrame({ &quot;sents&quot;: sents, &quot;labels&quot;: sentiments, &quot;topics&quot;: topics }) df.to_csv(file_path, index=False) return file_path . Define the UIT_VSFC datamodule class. You can read more here. . DS_URL = &quot;https://drive.google.com/uc?export=download&amp;id=1zg7cbRF2nFuJ2Q-AB63xlKuwEX3dTBsx&quot; class UIT_VSFC(pl.LightningDataModule): &quot;&quot;&quot; The Twitter dataset is ndwritten character digits derived from the NIST Special Database 19 &quot;&quot;&quot; def __init__(self, tokenizer, opts: Dict[str, Any]): super().__init__() self.tokenizer = tokenizer self.batch_size = opts[&#39;batch_size&#39;] self.num_workers = opts[&#39;num_workers&#39;] self.on_gpu = opts[&#39;on_gpu&#39;] self.data_collator = DataCollatorWithPadding(tokenizer=tokenizer) self.dataset = None self.mapping = {&quot;negative&quot;: 0, &quot;neutral&quot;: 1, &quot;positive&quot;: 2} self.inverse_mapping = {v: k for k, v in enumerate(self.mapping)} def prepare_data(self, *args, **kwargs) -&gt; None: data_dir = &#39;download/UIT_VSFC&#39; data_path = &#39;download/UIT_VSFC.zip&#39; if not os.path.exists(data_path): # Download the data data_path = download_url(DS_URL, &quot;UIT_VSFC.zip&quot;, &quot;download&quot;) if not os.path.exists(data_dir): # Unzip file with zipfile.ZipFile(data_path, &#39;r&#39;) as zip_ref: zip_ref.extractall(data_dir) # Load and save data to csv for path in [&quot;train&quot;, &quot;dev&quot;, &quot;test&quot;]: data = _load_data_from(f&quot;download/UIT_VSFC/{path}&quot;) if path == &quot;train&quot;: self.train_path = _save_to_csv(f&#39;{path}.csv&#39;, data) elif path == &quot;dev&quot;: self.dev_path = _save_to_csv(f&#39;{path}.csv&#39;, data) else: self.test_path = _save_to_csv(f&#39;{path}.csv&#39;, data) def setup(self, stage: str = None) -&gt; None: def encode(sample): return self.tokenizer(sample[&#39;sents&#39;], truncation=True) raw_datasets = load_dataset(&#39;csv&#39;, data_files={&#39;train&#39;: self.train_path, &#39;dev&#39;: self.dev_path, &#39;test&#39;: self.test_path}) self.dataset = raw_datasets.map(encode, batched=True) self.dataset = self.dataset.remove_columns( [&#39;sents&#39;, &#39;topics&#39;] ) self.dataset.set_format(&quot;torch&quot;) # Set the format of the datasets so they return PyTorch tensors instead of lists. def train_dataloader(self): return DataLoader( self.dataset[&#39;train&#39;], shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, collate_fn=self.data_collator ) def val_dataloader(self): return DataLoader( self.dataset[&#39;dev&#39;], shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, collate_fn=self.data_collator ) def test_dataloader(self): return DataLoader( self.dataset[&#39;test&#39;], shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, collate_fn=self.data_collator ) def __repr__(self): basic = f&quot;Twitter Dataset nNum classes: {len(self.mapping)} nMapping: {self.mapping} n&quot; if self.dataset is None: return basic batch = next(iter(self.train_dataloader())) data = ( f&quot;Train/val/test sizes: {len(self.dataset[&#39;train&#39;])}, {len(self.dataset[&#39;dev&#39;])}, {len(self.dataset[&#39;test&#39;])} n&quot; f&quot;Input_ids shape: {batch[&#39;input_ids&#39;].shape}&quot; ) return basic + data . Implementation . We will use the transformers model and wrapping it with the pytorch-lightning model class. This will help our code more clean and debug. You can read more about the pytorch-lightning model class here . class PhoBERT(pl.LightningModule): def __init__(self, lr, weight_decay): super().__init__() self.model = AutoModelForSequenceClassification.from_pretrained(&quot;vinai/phobert-base&quot;, num_labels=3) self.lr = lr self.weight_decay = weight_decay # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3) def configure_optimizers(self): return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay) def training_step(self, batch, batch_idx): outputs = self.model(**batch) loss, logits = outputs.loss, outputs.logits sentiments = batch[&#39;labels&#39;] scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): outputs = self.model(**batch) loss, logits = outputs.loss, outputs.logits sentiments = batch[&#39;labels&#39;] scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): outputs = self.model(**batch) logits = outputs.logits sentiments = batch[&#39;labels&#39;] scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . Training . tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/phobert-base&quot;) options = { &quot;on_gpu&quot;: True, &quot;batch_size&quot;: 32, &quot;num_workers&quot;: 4 } datamodule = UIT_VSFC(tokenizer, options) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # hyper-parameters lr = 2e-5 max_epochs = 10 weight_decay = 0.01 model = PhoBERT(lr, weight_decay) checkpoint_callback = ModelCheckpoint( monitor=&#39;val_f1&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, logger=tb_logger, callbacks=[checkpoint_callback], deterministic=True) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.1, max_epochs=max_epochs) #Debug trainer.fit(model, datamodule) . Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.decoder.bias&#39;, &#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;lm_head.decoder.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;roberta.pooler.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.dense.bias&#39;] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: [&#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;, &#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. GPU available: True, used: True TPU available: False, using: 0 TPU cores Using custom data configuration default-d20422fbdfea28fe . Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-d20422fbdfea28fe/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0... Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d20422fbdfea28fe/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data. . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . . | Name | Type | Params 0 | model | RobertaForSequenceClassification | 135 M 1 | train_acc | Accuracy | 0 2 | val_acc | Accuracy | 0 3 | val_f1 | F1 | 0 4 | test_acc | Accuracy | 0 5 | test_f1 | F1 | 0 135 M Trainable params 0 Non-trainable params 135 M Total params 540.002 Total estimated model params size (MB) Global seed set to 2401 . . trainer.test(ckpt_path=checkpoint_callback.best_model_path, test_dataloaders=datamodule.test_dataloader()) . Using custom data configuration default-d20422fbdfea28fe Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-d20422fbdfea28fe/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0) . . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.9314592480659485, &#39;test_f1&#39;: 0.9314592480659485} -- . [{&#39;test_acc&#39;: 0.9314592480659485, &#39;test_f1&#39;: 0.9314592480659485}] . Discussion . Results on test dataset: . Method Accuracy Macro-F1 . phoBERT | 0.931 | 0.931 | . MaxEnt (paper) | 87.9 | 87.9 | . We haven&#39;t tune the model but still get better result than the one in the UIT-VSFC paper. . To tune the model, there are somethings need to set up: . Wandb/Tensorboard: these tools will help us to visualize the loss per epoch of the model and other relevant information regarding metrics. Using those information we can come up with some ideas to tune the model. . | Wandb sweep: this tool allows us to define the range of hyperparameters we want to tune. . | . Lessons . When using transformers&#39; models, we should create out Dataset using the datasets library since it is helps to make the preprocessing step easier and cleaner. | Be careful with the metrics, for example, F1 micro and macro. | F1 and Accuracy are equal for cases in which every instance must be classified into one (and only one) class | .",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/06/21/NLP_3_PhoBERT_Sentiment_Analysis.html",
            "relUrl": "/implementation/2021/06/21/NLP_3_PhoBERT_Sentiment_Analysis.html",
            "date": " • Jun 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Effective LSTMs for Target Dependent Sentiment Classification [Part 2]",
            "content": "The full notebook is available here. . Install required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics . Import required packages . import pickle from collections import Counter, OrderedDict from pathlib import Path from typing import Any, Dict, List, Optional, Tuple, Union from urllib.request import urlretrieve import numpy as np from tqdm import tqdm import pytorch_lightning as pl import torch import torch.nn as nn import torch.nn.functional as F import torchmetrics import torchtext from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence, pad_sequence) from torch.utils.data import DataLoader, Dataset, random_split from torchtext.data import get_tokenizer from torchtext.vocab import Vectors, Vocab # For repoducibility pl.utilities.seed.seed_everything(seed=2401, workers=True) . Global seed set to 2401 . 2401 . Define dataset, data module class, utils function . =====Dataset File Format===== . Each instance consists three lines: . sentence (the target is replaced with $T$) | target | polarity label (0: neutral, 1:positive, -1:negative) | . Example: . i agree about arafat . i mean , shit , they even gave one to $T$ ha . it should be called &#39;&#39; the worst president &#39;&#39; prize . . jimmy carter . -1 . class TqdmUpTo(tqdm): &quot;&quot;&quot;From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py&quot;&quot;&quot; def update_to(self, blocks=1, bsize=1, tsize=None): &quot;&quot;&quot; Parameters - blocks: int, optional Number of blocks transferred so far [default: 1]. bsize: int, optional Size of each block (in tqdm units) [default: 1]. tsize: int, optional Total size (in tqdm units). If [default: None] remains unchanged. &quot;&quot;&quot; if tsize is not None: self.total = tsize # pylint: disable=attribute-defined-outside-init self.update(blocks * bsize - self.n) # will also set self.n = b * bsize class Tokenizer(): def __init__(self, tokenizer: Any): self.counter = Counter([&#39;&lt;pad&gt;&#39;, &#39;&lt;unk&gt;&#39;]) self.tokenizer = tokenizer self.vocab = self.update_vocab() def update_vocab(self): sorted_by_freq_tuples = sorted(self.counter.items(), key=lambda x: x[1], reverse=True) ordered_dict = OrderedDict(sorted_by_freq_tuples) self.vocab = torchtext.vocab.vocab(ordered_dict, min_freq=1) self.vocab.set_default_index(self.vocab[&#39;&lt;unk&gt;&#39;]) def fit_on_texts(self, texts: List[str]): &quot;&quot;&quot; Updates internal vocabulary based on a list of texts. &quot;&quot;&quot; # lower and tokenize texts to sequences for text in texts: self.counter.update(self.tokenizer(text)) # self.counter.update([t.lower().strip() for t in text.split()]) self.update_vocab() def texts_to_sequences(self, texts: List[str], reverse: bool=False, tensor: bool=True) -&gt; List[int]: sequences = [] for text in texts: seq = [self.vocab[word] for word in self.tokenizer(text)] if reverse: seq = seq[::-1] if tensor: seq = torch.tensor(seq) sequences.append(seq) return sequences def _load_data_from(path: Union[str, Path]) -&gt; Tuple[List[List[str]], List[List[str]], List[set]]: &quot;&quot;&quot; Create a dataset from a file path Return: a TwitterDataset object &quot;&quot;&quot; sentences = [] targets = [] sentiments = [] with open(path) as f: lines = f.readlines() # Read the file line by line and # check the relative index to parse the data according to the format. for i, line in enumerate(lines): index = i % 3 # compute the relative index if index == 0: sentences.append(line[:-1]) elif index == 1: targets.append(line[:-1]) elif index == 2: sentiments.append(line.strip()) return sentences, targets, sentiments def download_url(url, filename, directory=&#39;.&#39;): &quot;&quot;&quot;Download a file from url to filename, with a progress bar.&quot;&quot;&quot; if not os.path.exists(directory): os.makedirs(directory) path = os.path.join(directory, filename) with TqdmUpTo(unit=&quot;B&quot;, unit_scale=True, unit_divisor=1024, miniters=1) as t: urlretrieve(url, path, reporthook=t.update_to, data=None) # nosec return path def _preprocess_data(data, tokenizer): sents, targets, sentiments = data l_texts = [] r_texts = [] texts = [] for i, sent in enumerate(sents): l_text, _, r_text = sent.partition(&quot;$T$&quot;) l_text = l_text + &#39; &#39; + targets[i] r_text = targets[i] + &#39; &#39; + r_text text = l_text + &#39; &#39; + targets[i] + &#39; &#39; + r_text l_texts.append(l_text) r_texts.append(r_text) texts.append(text) # Generate left, right and target sequences l_sequences = tokenizer.texts_to_sequences(l_texts) r_sequences = tokenizer.texts_to_sequences(r_texts, reverse=True) target_sequences = tokenizer.texts_to_sequences(targets) sequences = tokenizer.texts_to_sequences(texts) # Calcuate length of each sequence in the left, right sequences l_lens = torch.tensor([len(seq) for seq in l_sequences]) r_lens = torch.tensor([len(seq) for seq in r_sequences]) lens = torch.tensor([len(seq) for seq in sequences]) # Padding sequences l_sequences = pad_sequence(l_sequences, batch_first=True) r_sequences = pad_sequence(r_sequences, batch_first=True) target_sequences = pad_sequence(target_sequences, batch_first=True) sequences = pad_sequence(sequences, batch_first=True) #Convert sentiment text to number sentiments = list(map(lambda x: int(x), sentiments)) sentiments = torch.tensor(sentiments) + 1 # increment labels by 1 # Double Checking assert len(r_lens) == len(r_sequences) assert len(l_lens) == len(l_sequences) assert len(l_lens) == len(sentiments) data = [] for i in range(len(sentiments)): sample = { &#39;padded_l_sequence&#39;: l_sequences[i], &#39;padded_r_sequence&#39;: r_sequences[i], &#39;padded_sequence&#39;: sequences[i], &#39;l_len&#39;: l_lens[i], &#39;r_len&#39;: r_lens[i], &#39;len&#39;: lens[i], &#39;padded_target_sequence&#39;: target_sequences[i], &#39;sentiment&#39;: sentiments[i] } data.append(sample) return data def build_vocab(tokenizer, data): sentences, targets = data texts = [] for i, sent in enumerate(sentences): texts.append(sent.replace(&#39;$T$&#39;, targets[i])) tokenizer.fit_on_texts(texts) def load_pretrained_word_embeddings(options: Dict[str, Any]): return torchtext.vocab.GloVe(options[&#39;name&#39;], options[&#39;dim&#39;]) def create_embedding_matrix(word_embeddings: Vectors, vocab: Vocab, path: Union[str, Path]): if os.path.exists(path): print(f&#39;loading embedding matrix from {path}&#39;) embedding_matrix = pickle.load(open(path, &#39;rb&#39;)) else: embedding_matrix = torch.zeros((len(vocab), word_embeddings.dim), dtype=torch.float) # words that are not availabel in the pretrained word embeddings will be zeros for word, index in vocab.get_stoi().items(): embedding_matrix[index] = word_embeddings.get_vecs_by_tokens(word) # save embedding matrix pickle.dump(embedding_matrix, open(path, &#39;wb&#39;)) return embedding_matrix . class TwitterDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] TRAIN_DS_URL = &quot;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/acl-14-short-data/train.raw&quot; TEST_DS_URL = &quot;https://raw.githubusercontent.com/songyouwei/ABSA-PyTorch/master/datasets/acl-14-short-data/test.raw&quot; class Twitter(pl.LightningDataModule): &quot;&quot;&quot; The Twitter dataset is ndwritten character digits derived from the NIST Special Database 19 &quot;&quot;&quot; def __init__(self, tokenizer: Tokenizer, opts: Dict[str, Any]): super().__init__() self.tokenizer = tokenizer self.batch_size = opts[&#39;batch_size&#39;] self.num_workers = opts[&#39;num_workers&#39;] self.on_gpu = opts[&#39;on_gpu&#39;] self.mapping = {&quot;negative&quot;: 0, &quot;neutral&quot;: 1, &quot;positive&quot;: 2} self.inverse_mapping = {v: k for k, v in enumerate(self.mapping)} def prepare_data(self, *args, **kwargs) -&gt; None: # Download the data train_path = &quot;download/raw_data/train.raw&quot; test_path = &quot;download/raw_data/test.raw&quot; if not os.path.exists(train_path): self.train_path = download_url(TRAIN_DS_URL, &quot;train.raw&quot;, &quot;download/raw_data&quot;) else: self.train_path = train_path if not os.path.exists(test_path): self.test_path = download_url(TEST_DS_URL, &quot;test.raw&quot;, &quot;download/raw_data&quot;) else: self.test_path = test_path def setup(self, stage: str = None) -&gt; None: if stage == &#39;fit&#39; or stage is None: # Load data from file train_data = _load_data_from(self.train_path) test_data = _load_data_from(self.test_path) # Preprocess data self.train_data = _preprocess_data(train_data, self.tokenizer) self.test_data = _preprocess_data(test_data, self.tokenizer) # In the paper, the author use the test set as validation set self.val_data = self.test_data elif stage == &#39;test&#39;: test_data = _load_data_from(self.test_path) self.test_data = _preprocess_data(test_data, self.tokenizer) def train_dataloader(self): # Create Dataset object train_ds = TwitterDataset(self.train_data) # Create Dataloader return DataLoader( train_ds, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def val_dataloader(self): val_ds = TwitterDataset(self.val_data) return DataLoader( val_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def test_dataloader(self): test_ds = TwitterDataset(self.test_data) return DataLoader( test_ds, shuffle=False, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=self.on_gpu, ) def __repr__(self): basic = f&quot;Twitter Dataset nNum classes: {len(self.mapping)} nMapping: {self.mapping} n&quot; if self.train_data is None and self.val_data is None and self.test_data is None: return basic x, y = next(iter(self.train_dataloader())) data = ( f&quot;Train/val/test sizes: {len(self.train_data)}, {len(self.val_data)}, {len(self.test_data)} n&quot; f&quot;Batch x stats: {(x.shape, x.dtype)} n&quot; f&quot;Batch y stats: {(y.shape, y.dtype)} n&quot; ) return basic + data . In the paper, the author trained the model on training set, and evaluated the performance on test set . Implement Model Architecture . We use Adam as our optimizer and using accuracy and f1 as our evaluating metrics, just like in the original paper. Also, we use cross entropy function to calculate our loss, which is the de-facto function for multi-class classification task. . TD-LSTM . The architecture has a embedding layer, 2 LSTM layers and 1 dense layer. . Embedding layer: | . Convert the sequences to word vectors using pre-trained Glove word embeddings . 2 LSTM layers: | . One layer is used for the [left context + target] sequences, and one is used for the [target + right context] sequences. . Dense layer: | . We concate the 2 hidden states from the LSTM layers and feed it into the Dense layer. . To take into account of the target information, we make a slight modification on the $LSTM$ model. The basic idea is to model the preceding and following contexts surrounding the target string, so that contexts in both directions could be used as feature representations for sentiment classification. We believe that capturing such target-dependent context information could improve the accuracy of target-dependent sentiment classification. . Specifically, we use two $LSTM$ neural networks, a left one $LSTM_L$ and a right one $LSTM_R$, to model the preceding and following contexts respectively. An illustration of the model is shown in Figure 1. The input of $LSTM_L$ is the preceding contexts plus target string, and the input of $LSTM_R$ is the following contexts plus target string. We run $LSTM_L$ from left to right, and run $LSTM_R$ from right to left. We favor this strategy as we believe that regarding target string as the last unit could better utilize the semantics of target string when using the composed representation for sentiment classification. Afterwards, we concatenate the last hidden vectors of $LSTM_L$ and $LSTM_R$ , and feed them to a sof tmax layer to classify the sentiment polarity label. One could also try averaging or summing the last hidden vectors of $LSTM_L$ and $LSTM_R$ as alternatives. . from IPython.display import Image Image(filename=&#39;images/figure_1_image.png&#39;) . class TDLSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.l_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.r_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear = nn.Linear(hidden_size*2, num_classes) self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, data): cols = [&#39;padded_l_sequence&#39;, &#39;padded_r_sequence&#39;, &#39;l_len&#39;, &#39;r_len&#39;] padded_l_seqs, padded_r_seqs, l_lens, r_lens = [data[col] for col in cols] # convert seq to word vector padded_l_embeds = self.embedding(padded_l_seqs) padded_r_embeds = self.embedding(padded_r_seqs) # pack the embeds padded_l_seq_pack = pack_padded_sequence(padded_l_embeds, l_lens.cpu(), batch_first=True, enforce_sorted=False) padded_r_seq_pack = pack_padded_sequence(padded_r_embeds, r_lens.cpu(), batch_first=True, enforce_sorted=False) _, (h_l, _) = self.l_lstm(padded_l_seq_pack) _, (h_r, _) = self.r_lstm(padded_r_seq_pack) h = torch.cat((h_l[-1], h_r[-1]), -1) # B x 2H out = self.linear(h) return out def training_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . TC-LSTM . The architecture has a embedding layer, 2 LSTM layers and 1 dense layer. . Embedding layer: | . Convert the sequences to word vectors using pre-trained Glove word embeddings . 2 LSTM layers: | . One layer is used for the [left context + target] sequences, and one is used for the [target + right context] sequences. . Dense layer: | . We concate the 2 hidden states from the LSTM layers and feed it into the Dense layer. . The only difference compared to the TD-LSTM is its input. The input of TC-LSTM is a concatenation of the input word vector and the $v_{target}$ vector. We calculate the $v_{target}$ vector by averaging the all the target word vector(s) of the sample. For example, if the target in the sentence is jimmy carter, we tokenizer the target to jimmy and carter then convert them to word vector. After that, we average those vector to get the $v_{target}$ vector. . An overview of TC-LSTM is illustrated in Figure 2. The model extends TD-LSTM by incorporating an target con- nection component, which explicitly utilizes the connections between target word and each context word when composing the representation of a sentence. . The input of TC-LSTM is a sentence consist- ing of n words { $w_1,w_2,...w_n$ } and a target string t occurs in the sentence. We represent target t as { $w_{l+1}, w_{l+2}...w_{r−1}$ } because a target could be a word sequence of variable length, such as “google” or “harry potter”. When processing a sentence, we split it into three components:target words, preceding context words and following context words. We obtain target vector $v_{target}$ by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities (Socher et al., 2013a; Sun et al., 2015). When compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in TD-LSTM. The difference is that in TC-LSTM the input at each position is the concatenation of word embedding and target vector vtarget, while in TD-LSTM the input at each position only includes only the embedding of current word. The input data has an additional element which is the $v_{target}$ vector. Let create a new Dataset class for TC-LSTM. . from IPython.display import Image Image(filename=&#39;images/figure_2_image.png&#39;) . class TCLSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.l_lstm = nn.LSTM(embedding_dim*2, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.r_lstm = nn.LSTM(embedding_dim*2, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear = nn.Linear(hidden_size*2, num_classes) self.lr = lr self.l2reg = l2reg # log hyperparameters # self.save_hyperparameters() # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, data): cols = [&#39;padded_l_sequence&#39;, &#39;padded_r_sequence&#39;, &#39;l_len&#39;, &#39;r_len&#39;, &#39;padded_target_sequence&#39;] padded_l_seqs, padded_r_seqs, l_lens, r_lens, padded_target_seqs = [data[col] for col in cols] # convert seq to word vector padded_l_embeds = self.embedding(padded_l_seqs) padded_r_embeds = self.embedding(padded_r_seqs) padded_target_embeds = self.embedding(padded_target_seqs) # BxLxH # create v_target vector and concat it to both l_embeds and r_embeds v_targets = torch.mean(padded_target_embeds, dim=1, keepdims=True) padded_l_embeds = torch.cat((padded_l_embeds, v_targets.expand((-1, padded_l_embeds.shape[1], -1))), dim=2) padded_r_embeds = torch.cat((padded_r_embeds, v_targets.expand((-1, padded_r_embeds.shape[1], -1))), dim=2) # pack the embeds padded_l_seq_pack = pack_padded_sequence(padded_l_embeds, l_lens.cpu(), batch_first=True, enforce_sorted=False) padded_r_seq_pack = pack_padded_sequence(padded_r_embeds, r_lens.cpu(), batch_first=True, enforce_sorted=False) _, (h_l, _) = self.l_lstm(padded_l_seq_pack) _, (h_r, _) = self.r_lstm(padded_r_seq_pack) h = torch.cat((h_l[-1], h_r[-1]), -1) # B x 2H out = self.linear(h) return out def training_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=True, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . LSTM . This is just a simple LSTM model with a embedding layer, 1 LSTM layers and 1 dense layer. . For the input data, we simply feed all the input word vector to the LSTM without informing the model any information of the target words. . The LSTM model solves target-dependent sentiment classification in a target- independent way. That is to say, the feature representation used for sentiment classification remains the same without considering the target words. Let us again take “I bought a new camera. The picture quality is amazing but the battery life is too short” as an example. The representations of this sentence with regard to picture quality and battery life are identical. This is evidently problematic as the sentiment polarity labels towards these two targets are different. . from IPython.display import Image Image(filename=&#39;images/figure_3_image.png&#39;) . class LSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear = nn.Linear(hidden_size, num_classes) self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, data): cols = [&#39;padded_sequence&#39;, &#39;len&#39;] padded_seqs, lens = [data[col] for col in cols] # convert seq to word vector padded_embeds = self.embedding(padded_seqs) # pack the embeds padded_seq_pack = pack_padded_sequence(padded_embeds, lens.cpu(), batch_first=True, enforce_sorted=False) _, (h, _) = self.lstm(padded_seq_pack) out = self.linear(h[-1]) return out def training_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument sentiments = batch[&#39;sentiment&#39;] logits = self.forward(batch) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . Training . First of all we will load the pre-trained word embedding Glove. We use the same one with the author. . We use 100-dimensional Glove vectors learned from Twitter, randomize the parameters with uniform distribution U(−0.003,0.003), set the clipping threshold of softmax layer as 200 and set learning rate as 0.01. . Since the author does not provide explicitly the hyper-parameters he used, we have to fine-tune a bit to get good result. . word_embeddings = load_pretrained_word_embeddings({&quot;name&quot;: &quot;twitter.27B&quot;, &quot;dim&quot;: 100}) . .vector_cache/glove.twitter.27B.zip: 1.52GB [04:53, 5.18MB/s] 100%|█████████▉| 1191916/1193514 [00:43&lt;00:00, 27135.96it/s] . download_url(TRAIN_DS_URL, &quot;train.raw&quot;, &quot;download/raw_data&quot;) download_url(TEST_DS_URL, &quot;test.raw&quot;, &quot;download/raw_data&quot;) train_data = _load_data_from(&quot;download/raw_data/train.raw&quot;) test_data = _load_data_from(&quot;download/raw_data/test.raw&quot;) # Build vocabulary for the dataset all_sentences = train_data[0] + test_data[0] all_targets = train_data[1] + test_data[1] tokenizer = Tokenizer(get_tokenizer(&quot;basic_english&quot;)) build_vocab(tokenizer, [all_sentences, all_targets]) # Create datamodule options = { &quot;on_gpu&quot;: True, &quot;batch_size&quot;: 64, &quot;num_workers&quot;: 2 } datamodule = Twitter(tokenizer, options) # Create embedding matrix embedding_matrix = create_embedding_matrix(word_embeddings, tokenizer.vocab, &quot;embedding_matrix.dat&quot;) . TD-LSTM . checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 num_epochs = 30 l2reg = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback], deterministic=True) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.1, max_epochs=30) #Debug model = TDLSTM(embedding_matrix, hidden_size, lr=lr, l2reg=l2reg) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores . loading embedding matrix from embedding_matrix.dat . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 1.3 M 1 | l_lstm | LSTM | 482 K 2 | r_lstm | LSTM | 482 K 3 | linear | Linear | 1.8 K 4 | train_acc | Accuracy | 0 5 | val_acc | Accuracy | 0 6 | val_f1 | F1 | 0 7 | test_acc | Accuracy | 0 8 | test_f1 | F1 | 0 - 966 K Trainable params 1.3 M Non-trainable params 2.3 M Total params 9.235 Total estimated model params size (MB) Global seed set to 2401 . . new_model = TDLSTM.load_from_checkpoint(checkpoint_callback.best_model_path, embeddings=embedding_matrix, hidden_size=300) trainer.test(new_model, datamodule.test_dataloader()) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.6979768872261047, &#39;test_f1&#39;: 0.6850955486297607} -- . [{&#39;test_acc&#39;: 0.6979768872261047, &#39;test_f1&#39;: 0.6850955486297607}] . TC-LSTM . checkpoint_callback_2 = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 num_epochs = 30 l2reg = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback_2]) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.1, max_epochs=30) #Debug model = TCLSTM(embedding_matrix, hidden_size, lr=lr, l2reg=l2reg) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 1.3 M 1 | l_lstm | LSTM | 602 K 2 | r_lstm | LSTM | 602 K 3 | linear | Linear | 1.8 K 4 | train_acc | Accuracy | 0 5 | val_acc | Accuracy | 0 6 | val_f1 | F1 | 0 7 | test_acc | Accuracy | 0 8 | test_f1 | F1 | 0 - 1.2 M Trainable params 1.3 M Non-trainable params 2.5 M Total params 10.195 Total estimated model params size (MB) Global seed set to 2401 . . new_model = TCLSTM.load_from_checkpoint(checkpoint_callback_2.best_model_path, embeddings=embedding_matrix, hidden_size=300) trainer.test(new_model, datamodule.test_dataloader()) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.7008670568466187, &#39;test_f1&#39;: 0.6788402199745178} -- . [{&#39;test_acc&#39;: 0.7008670568466187, &#39;test_f1&#39;: 0.6788402199745178}] . LSTM . checkpoint_callback_3 = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # Set hyper-parameters lr = 1e-3 hidden_size = 300 num_epochs = 30 l2reg = 0.0 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback_3]) # trainer = pl.Trainer(fast_dev_run=True) #Debug # trainer = pl.Trainer(overfit_batches=0.1, max_epochs=30) #Debug model = LSTM(embedding_matrix, hidden_size, lr=lr, l2reg=l2reg) trainer.fit(model, datamodule) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 1.3 M 1 | lstm | LSTM | 482 K 2 | linear | Linear | 903 3 | train_acc | Accuracy | 0 4 | val_acc | Accuracy | 0 5 | val_f1 | F1 | 0 6 | test_acc | Accuracy | 0 7 | test_f1 | F1 | 0 - 483 K Trainable params 1.3 M Non-trainable params 1.8 M Total params 7.302 Total estimated model params size (MB) Global seed set to 2401 . . new_model = LSTM.load_from_checkpoint(checkpoint_callback_3.best_model_path, embeddings=embedding_matrix, hidden_size=300) trainer.test(new_model, datamodule.test_dataloader()) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.6878612637519836, &#39;test_f1&#39;: 0.6633064150810242} -- . [{&#39;test_acc&#39;: 0.6878612637519836, &#39;test_f1&#39;: 0.6633064150810242}] . Discussion . Our result: . Method Accuracy Macro-F1 . LSTM | 0.687 | 0.66 | . TD-LSTM | 0.697 | 0.685 | . TC-LSTM | 0.7 | 0.679 | . Paper result: . Method Accuracy Macro-F1 . LSTM | 0.665 | 0.647 | . TD-LSTM | 0.708 | 0.690 | . TC-LSTM | 0.715 | 0.695 | . Firstly, compared to the result from the paper, our implementation gets very close results. You can try to tune the model to get a better result. . Secondly, it is surprising that we can get a much better result with the simple LSTM model compared to the paper result. The reason that the LSTM can get a very close result compared to TD-LSTM and TC-LSTM is explainable. Even though this is the target-dependent sentiment classification task, there is only one target per sentence in the dataset. Therefore, the target information is redundant in this case. The LSTM model can use the surrounding words to classify the sentence. . You can read more about the paper here . from IPython.display import Image Image(filename=&#39;images/results.png&#39;) . Lessons . Even though the embedding layer is frozen during traning (parameters not updated), using the corpus vocab to create embedding matrix from pretrained Glove yield better result than using the whole word embeddings for the embedding layer. . | Using pad_sequence and pack_padded_sequence assure the LSTM/RNN/GRU not processing the padding token. It is better than padding with max length. The result of 2 methods are the same. From what I search, padding with max length will adversely affect the performance of the model. Even though, we can set the loss function to not to account for the padding token, the padding token still have affect on the input tokens. The reason may be that the latter will process the padding token together with the input ones. . | Consider the structure of the project before coding it to save refactoring time. . |",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/06/20/NLP_2_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-2.html",
            "relUrl": "/implementation/2021/06/20/NLP_2_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-2.html",
            "date": " • Jun 20, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Effective LSTMs for Target Dependent Sentiment Classification [Part 1]",
            "content": "from IPython.display import Image Image(filename=&#39;images/paper_image.png&#39;) . Target-Dependent Sentiment Classification is one of the text classification problems in the field of sentiment analysis. Given a sentence and a target to the model, it has to output the sentiment polarity (e.g positive, negative, neutral) of the sentence towards that target. For example, we have a sentence &quot;I bout a new camera. The pucture quality is amazing but the battery life is too short&quot;. If we input the target picture quality, we expect the sentiment to be &quot;positive&quot;. On the other hand, if we input the target battery life, we expect the sentiment to be &quot;negative&quot;. . The author argues that the Target-Dependent sentiment classification is challenging since it is hard to effectively model the sentiment relatedness of a target word with its context words in a sentence. Doing feature engineerings are clumsy, so they propose a neural network approach with 2 models Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM(TC-LSTM). . In this post, I will implement those models and compare it with the plain LSTM model, just like they did. Yet, I will not cover other approaches using SVM and RNN. Since in the original paper, the author did not provide the specific hyper-parameters they used for their models, I will fine-tune it on my own. . This post covers the data processing step and the implementation of TD-LSTM. The second post will cover the implementation of TC-LSTM and comparision between three models: TC-LSTM, TD-LSTM, and LSTM. . The full notebook is available here. . Install required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics # !pip install transformers . Download dataset and pretrained word-embedding . First of all you should download the dataset. The dataset used in the paper is from the Twitter (Dong et al., 2014). You can download from here. After downloading, you should unzip the dataset file in the same folder with the notebook. They should be in the same folder to run properly. . %%capture !unzip acl-14-short-data.zip . In the paper, the author used the 100-dimensional Glove vectors learned from Twitter. Download the word embedding file and unzip it in the same folder with the notebook. . %%capture !wget https://nlp.stanford.edu/data/glove.twitter.27B.zip !unzip glove.twitter.27B.zip . Import required packages . import numpy as np import pytorch_lightning as pl import torch import torch.nn as nn import torch.nn.functional as F import torchmetrics from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.utils.data import DataLoader, Dataset, random_split from torchtext.data import get_tokenizer . Load dataset from file and create dataloaders . =====Dataset File Format===== . Each instance consists three lines: . sentence (the target is replaced with $T$) | target | polarity label (0: neutral, 1:positive, -1:negative) | . Example: . i agree about arafat . i mean , shit , they even gave one to $T$ ha . it should be called &#39;&#39; the worst president &#39;&#39; prize . . jimmy carter . -1 . Target-Dependent LSTM (TD-LSTM) . The LSTM model solves target-dependent sentiment classification in a target- independent way. That is to say, the feature representation used for sentiment classification remains the same without considering the target words. Let us again take “I bought a new camera. The picture quality is amazing but the battery life is too short” as an example. The representations of this sentence with regard to picture quality and battery life are identical. This is evidently problematic as the sentiment polarity labels towards these two targets are different. . To take into account of the target information, we make a slight modification on the aforementioned LSTM model and introduce a target-dependent LSTM (TD-LSTM) in this subsection. The basic idea is to model the preceding and following contexts surrounding the target string, so that contexts in both directions could be used as feature representations for sentiment classification. We believe that capturing such target-dependent context information could improve the accuracy of target-dependent sentiment classification. . Specifically, we use two LSTM neural networks, a left one LSTML and a right one LSTMR, to model the preceding and following contexts respectively. An illustration of the model is shown in Figure 1. The input of LSTML is the preceding contexts plus target string, and the input of LSTMR is the following contexts plus target string. We run LSTML from left to right, and run LSTMR from right to left. We favor this strategy as we believe that regarding target string as the last unit could better utilize the semantics of target string when using the composed representation for sentiment classification. Afterwards, we concatenate the last hidden vectors of LSTML and LSTMR , and feed them to a sof tmax layer to classify the sentiment polarity label. One could also try averaging or summing the last hidden vectors of LSTML and LSTMR as alternatives. . from IPython.display import Image Image(filename=&#39;images/firgure_1_image.png&#39;) . class TwitterTDLSTMDataset(Dataset): def __init__(self, l_sequences, r_sequences, l_lens, r_lens, sentiments): self.l_sequences = l_sequences self.r_sequences = r_sequences self.l_lens = l_lens self.r_lens = r_lens self.sentiments = sentiments def __len__(self): return len(self.sentiments) def __getitem__(self, idx): return (self.l_sequences[idx], self.l_lens[idx]), (self.r_sequences[idx], self.r_lens[idx]), self.sentiments[idx] . def create_dataset_from(path: str): &quot;&quot;&quot; Create a dataset from a file path Return: a TwitterDataset object &quot;&quot;&quot; sentences = [] targets = [] sentiments = [] with open(path) as f: lines = f.readlines() # Read the file line by line and # check the relative index to parse the data according to the format. for i, line in enumerate(lines): index = i % 3 # compute the relative index if index == 0: sentences.append(line[:-1]) elif index == 1: targets.append(line[:-1]) elif index == 2: sentiments.append(line.strip()) #Load tokenizer tokenizer = get_tokenizer(&quot;basic_english&quot;) #Tokenize and Lower sentence and target text tokenized_sentences = list(map(lambda x: tokenizer(x), sentences)) targets = list(map(lambda x: tokenizer(x), targets)) #Convert sentiment text to number sentiments = list(map(lambda x: int(x), sentiments)) #Generate sequence_l, sequence_r l_sequences = [] r_sequences = [] for i, sent in enumerate(tokenized_sentences): seq_l, seq_r = [], [] flag = True for token in sent: if word_2_id.get(token) == len(word_2_id) - 1: flag = False continue if flag: # get the index of the token in the vocab # if the token does not exists in the vocab, return index of &lt;UNK&gt; token seq_l.append(word_2_id.get(token, 1)) else: seq_r.append(word_2_id.get(token, 1)) target_seq = [word_2_id.get(token, 1) for token in targets[i]] seq_l = torch.tensor(seq_l + target_seq) seq_r = torch.tensor((target_seq + seq_r)[::-1]) # reverse the seq_r l_sequences.append(seq_l) r_sequences.append(seq_r) l_lens = torch.tensor([len(seq) for seq in l_sequences]) r_lens = torch.tensor([len(seq) for seq in r_sequences]) sentiments = torch.tensor(sentiments) + 1 assert len(l_lens) == len(l_sequences) assert len(r_lens) == len(r_sequences) assert len(l_lens) == len(sentiments) return TwitterTDLSTMDataset(l_sequences, r_sequences, l_lens, r_lens, sentiments) . def load_w2v(embedding_file_path: str): &quot;&quot;&quot; Load pretrained word-embeddings from a file path Return a word_2_id dictionary and a embedding matrix &quot;&quot;&quot; word_2_id = {&#39;&lt;PAD&gt;&#39;: 0, &#39;&lt;UNK&gt;&#39;: 1} embeddings = [torch.zeros(100), torch.zeros(100)] with open(embedding_file_path) as f: for i, line in enumerate(f.readlines()): tokens = line.split() word, vec = &#39; &#39;.join(tokens[:-100]), tokens[-100:] word_2_id[word] = i + 2 # convert list of str to float float_tokens = np.array(vec, dtype=float) embeddings.append(torch.tensor(float_tokens, dtype=torch.float)) embeddings = torch.stack(embeddings) embeddings[word_2_id[&#39;&lt;UNK&gt;&#39;]] = torch.mean(embeddings[2:], dim=0) word_2_id[&#39;$t$&#39;] = len(word_2_id) return word_2_id, embeddings . word_2_id, embeddings = load_w2v(&quot;glove.twitter.27B.100d.txt&quot;) . from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence def collate_batch(batch): &quot;&quot;&quot; Combine samples from dataset into a batch &quot;&quot;&quot; l_sequences = [] l_lens = [] r_sequences = [] r_lens = [] sentiments = [] for (l_sequence, l_len), (r_sequence, r_len), sentiment in batch: l_sequences.append(l_sequence) l_lens.append(l_len) r_sequences.append(r_sequence) r_lens.append(r_len) sentiments.append(sentiment) padded_l_seq = pad_sequence(l_sequences, batch_first=True, padding_value=0) padded_r_seq = pad_sequence(r_sequences, batch_first=True, padding_value=0) return (padded_l_seq, l_lens), (padded_r_seq, r_lens), torch.tensor(sentiments) . In the paper, the author trained the model on training set, and evaluated the performance on test set . dataset = create_dataset_from(&quot;/content/acl-14-short-data/train.raw&quot;) dataloaders = DataLoader(dataset, batch_size=128, collate_fn=collate_batch) . test_dataset = create_dataset_from(&quot;/content/acl-14-short-data/test.raw&quot;) test_dataloaders = DataLoader(test_dataset, batch_size=64, collate_fn=collate_batch) . Implement Model Architecture . The architecture has a embedding layer, 2 LSTM layers and 1 dense layer. . Embedding layer: | . Convert the sequences to word vectors using pre-trained Glove word embeddings . 2 LSTM layers: | . One layer is used for the [left context + target] sequences, and one is used for the [target + right context] sequences. . Dense layer: | . We concate the 2 hidden states from the LSTM layers and feed it into the Dense layer. . Notes: . We use Adam as our optimizer and using accuracy and f1 as our evaluating metrics, just like in the original paper. . class TDLSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.l_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.r_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear = nn.Linear(hidden_size*2, num_classes) self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, padded_l_seqs, l_lens, padded_r_seqs, r_lens): # convert seq to word vector padded_l_embeds = self.embedding(padded_l_seqs) padded_r_embeds = self.embedding(padded_r_seqs) # pack the embeds padded_l_seq_pack = pack_padded_sequence(padded_l_embeds, l_lens, batch_first=True, enforce_sorted=False) padded_r_seq_pack = pack_padded_sequence(padded_r_embeds, r_lens, batch_first=True, enforce_sorted=False) _, (h_l, _) = self.l_lstm(padded_l_seq_pack) _, (h_r, _) = self.r_lstm(padded_r_seq_pack) h = torch.cat((h_l[-1], h_r[-1]), -1) # B x 2H out = self.linear(h) return out def training_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . Training . checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, filename=&#39;best_model&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # hyper-parameters lr = 1e-3 hidden_size = 500 num_epochs = 60 l2reg = 0.5 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback]) model = TDLSTM(embeddings, hidden_size, lr=lr, l2reg=l2reg) trainer.fit(model, dataloaders, test_dataloaders) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 119 M 1 | l_lstm | LSTM | 1.2 M 2 | r_lstm | LSTM | 1.2 M 3 | linear | Linear | 3.0 K 4 | train_acc | Accuracy | 0 5 | val_acc | Accuracy | 0 6 | val_f1 | F1 | 0 7 | test_acc | Accuracy | 0 8 | test_f1 | F1 | 0 - 2.4 M Trainable params 119 M Non-trainable params 121 M Total params 487.050 Total estimated model params size (MB) . . new_model = TDLSTM.load_from_checkpoint(checkpoint_callback.best_model_path, embeddings=embeddings, hidden_size=500) trainer.test(new_model, test_dataloaders) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.7037572264671326, &#39;test_f1&#39;: 0.6847572326660156} -- . [{&#39;test_acc&#39;: 0.7037572264671326, &#39;test_f1&#39;: 0.6847572326660156}] . from IPython.display import Image Image(filename=&#39;images/results.png&#39;) . Compare to the result from the paper, our implementation gets very close results. You can try to tune the model to get better result. .",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2021/06/18/NLP_1_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-1.html",
            "relUrl": "/implementation/2021/06/18/NLP_1_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-1.html",
            "date": " • Jun 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://minhdang241.github.io/minhdg-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://minhdang241.github.io/minhdg-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, Im Minh. Welcome to my NLP365 project. This is where I document all the things I have learned and researched about NLP and ML in general. .",
          "url": "https://minhdang241.github.io/minhdg-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://minhdang241.github.io/minhdg-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}