{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://minhdang241.github.io/minhdg-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Effective LSTMs for Target Dependent Sentiment Classification [Part 1]",
            "content": "from IPython.display import Image Image(filename=&#39;images/paper_image.png&#39;) . Target-Dependent Sentiment Classification is one of the text classification problems in the field of sentiment analysis. Given a sentence and a target to the model, it has to output the sentiment polarity (e.g positive, negative, neutral) of the sentence towards that target. For example, we have a sentence &quot;I bout a new camera. The pucture quality is amazing but the battery life is too short&quot;. If we input the target picture quality, we expect the sentiment to be &quot;positive&quot;. On the other hand, if we input the target battery life, we expect the sentiment to be &quot;negative&quot;. . The author argues that the Target-Dependent sentiment classification is challenging since it is hard to effectively model the sentiment relatedness of a target word with its context words in a sentence. Doing feature engineerings are clumsy, so they propose a neural network approach with 2 models Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM(TC-LSTM). . In this post, I will implement those models and compare it with the plain LSTM model, just like they did. Yet, I will not cover other approaches using SVM and RNN. Since in the original paper, the author did not provide the specific hyper-parameters they used for their models, I will fine-tune it on my own. . This post covers the data processing step and the implementation of TD-LSTM. The second post will cover the implementation of TC-LSTM and comparision between three models: TC-LSTM, TD-LSTM, and LSTM. . The full notebook is available here. . Install required packages . %%capture !pip install pytorch-lightning !pip install torchmetrics # !pip install transformers . Download dataset and pretrained word-embedding . First of all you should download the dataset. The dataset used in the paper is from the Twitter (Dong et al., 2014). You can download from here. After downloading, you should unzip the dataset file in the same folder with the notebook. They should be in the same folder to run properly. . %%capture !unzip acl-14-short-data.zip . In the paper, the author used the 100-dimensional Glove vectors learned from Twitter. Download the word embedding file and unzip it in the same folder with the notebook. . %%capture !wget https://nlp.stanford.edu/data/glove.twitter.27B.zip !unzip glove.twitter.27B.zip . Import required packages . import numpy as np import pytorch_lightning as pl import torch import torch.nn as nn import torch.nn.functional as F import torchmetrics from pytorch_lightning import loggers as pl_loggers from pytorch_lightning.callbacks import ModelCheckpoint from torch.utils.data import DataLoader, Dataset, random_split from torchtext.data import get_tokenizer . Load dataset from file and create dataloaders . =====Dataset File Format===== . Each instance consists three lines: . sentence (the target is replaced with $T$) | target | polarity label (0: neutral, 1:positive, -1:negative) | . Example: . i agree about arafat . i mean , shit , they even gave one to $T$ ha . it should be called &#39;&#39; the worst president &#39;&#39; prize . . jimmy carter . -1 . Target-Dependent LSTM (TD-LSTM) . The aforementioned LSTM model solves target-dependent sentiment classification in a target- independent way. That is to say, the feature representation used for sentiment classification remains the same without considering the target words. Let us again take “I bought a new camera. The picture quality is amazing but the battery life is too short” as an example. The representations of this sentence with regard to picture quality and battery life are identical. This is evidently problematic as the sentiment polarity labels towards these two targets are different. . To take into account of the target information, we make a slight modification on the aforementioned LSTM model and introduce a target-dependent LSTM (TD-LSTM) in this subsection. The basic idea is to model the preceding and following contexts surrounding the target string, so that contexts in both directions could be used as feature representations for sentiment classification. We believe that capturing such target-dependent context information could improve the accuracy of target-dependent sentiment classification. . Specifically, we use two LSTM neural networks, a left one LSTML and a right one LSTMR, to model the preceding and following contexts respectively. An illustration of the model is shown in Figure 1. The input of LSTML is the preceding contexts plus target string, and the input of LSTMR is the following contexts plus target string. We run LSTML from left to right, and run LSTMR from right to left. We favor this strategy as we believe that regarding target string as the last unit could better utilize the semantics of target string when using the composed representation for sentiment classification. Afterwards, we concatenate the last hidden vectors of LSTML and LSTMR , and feed them to a sof tmax layer to classify the sentiment polarity label. One could also try averaging or summing the last hidden vectors of LSTML and LSTMR as alternatives. . from IPython.display import Image Image(filename=&#39;images/firgure_1_image.png&#39;) . class TwitterTDLSTMDataset(Dataset): def __init__(self, l_sequences, r_sequences, l_lens, r_lens, sentiments): self.l_sequences = l_sequences self.r_sequences = r_sequences self.l_lens = l_lens self.r_lens = r_lens self.sentiments = sentiments def __len__(self): return len(self.sentiments) def __getitem__(self, idx): return (self.l_sequences[idx], self.l_lens[idx]), (self.r_sequences[idx], self.r_lens[idx]), self.sentiments[idx] . def create_dataset_from(path: str): &quot;&quot;&quot; Create a dataset from a file path Return: a TwitterDataset object &quot;&quot;&quot; sentences = [] targets = [] sentiments = [] with open(path) as f: lines = f.readlines() # Read the file line by line and # check the relative index to parse the data according to the format. for i, line in enumerate(lines): index = i % 3 # compute the relative index if index == 0: sentences.append(line[:-1]) elif index == 1: targets.append(line[:-1]) elif index == 2: sentiments.append(line.strip()) #Load tokenizer tokenizer = get_tokenizer(&quot;basic_english&quot;) #Tokenize and Lower sentence and target text tokenized_sentences = list(map(lambda x: tokenizer(x), sentences)) targets = list(map(lambda x: tokenizer(x), targets)) #Convert sentiment text to number sentiments = list(map(lambda x: int(x), sentiments)) #Generate sequence_l, sequence_r l_sequences = [] r_sequences = [] for i, sent in enumerate(tokenized_sentences): seq_l, seq_r = [], [] flag = True for token in sent: if word_2_id.get(token) == len(word_2_id) - 1: flag = False continue if flag: # get the index of the token in the vocab # if the token does not exists in the vocab, return index of &lt;UNK&gt; token seq_l.append(word_2_id.get(token, 1)) else: seq_r.append(word_2_id.get(token, 1)) target_seq = [word_2_id.get(token, 1) for token in targets[i]] seq_l = torch.tensor(seq_l + target_seq) seq_r = torch.tensor((target_seq + seq_r)[::-1]) # reverse the seq_r l_sequences.append(seq_l) r_sequences.append(seq_r) l_lens = torch.tensor([len(seq) for seq in l_sequences]) r_lens = torch.tensor([len(seq) for seq in r_sequences]) sentiments = torch.tensor(sentiments) + 1 assert len(l_lens) == len(l_sequences) assert len(r_lens) == len(r_sequences) assert len(l_lens) == len(sentiments) return TwitterTDLSTMDataset(l_sequences, r_sequences, l_lens, r_lens, sentiments) . def load_w2v(embedding_file_path: str): &quot;&quot;&quot; Load pretrained word-embeddings from a file path Return a word_2_id dictionary and a embedding matrix &quot;&quot;&quot; word_2_id = {&#39;&lt;PAD&gt;&#39;: 0, &#39;&lt;UNK&gt;&#39;: 1} embeddings = [torch.zeros(100), torch.zeros(100)] with open(embedding_file_path) as f: for i, line in enumerate(f.readlines()): tokens = line.split() word, vec = &#39; &#39;.join(tokens[:-100]), tokens[-100:] word_2_id[word] = i + 2 # convert list of str to float float_tokens = np.array(vec, dtype=float) embeddings.append(torch.tensor(float_tokens, dtype=torch.float)) embeddings = torch.stack(embeddings) embeddings[word_2_id[&#39;&lt;UNK&gt;&#39;]] = torch.mean(embeddings[2:], dim=0) word_2_id[&#39;$t$&#39;] = len(word_2_id) return word_2_id, embeddings . word_2_id, embeddings = load_w2v(&quot;glove.twitter.27B.100d.txt&quot;) . from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence def collate_batch(batch): &quot;&quot;&quot; Combine samples from dataset into a batch &quot;&quot;&quot; l_sequences = [] l_lens = [] r_sequences = [] r_lens = [] sentiments = [] for (l_sequence, l_len), (r_sequence, r_len), sentiment in batch: l_sequences.append(l_sequence) l_lens.append(l_len) r_sequences.append(r_sequence) r_lens.append(r_len) sentiments.append(sentiment) padded_l_seq = pad_sequence(l_sequences, batch_first=True, padding_value=0) padded_r_seq = pad_sequence(r_sequences, batch_first=True, padding_value=0) return (padded_l_seq, l_lens), (padded_r_seq, r_lens), torch.tensor(sentiments) . In the paper, the author trained the model on training set, and evaluated the performance on test set . dataset = create_dataset_from(&quot;/content/acl-14-short-data/train.raw&quot;) dataloaders = DataLoader(dataset, batch_size=128, collate_fn=collate_batch) . test_dataset = create_dataset_from(&quot;/content/acl-14-short-data/test.raw&quot;) test_dataloaders = DataLoader(test_dataset, batch_size=64, collate_fn=collate_batch) . Implement Model Architecture . The architecture has a embedding layer, 2 LSTM layers and 1 dense layer. . Embedding layer: | . Convert the sequences to word vectors using pre-trained Glove word embeddings . 2 LSTM layers: | . One layer is used for the [left context + target] sequences, and one is used for the [target + right context] sequences. . Dense layer: | . We concate the 2 hidden states from the LSTM layers and feed it into the Dense layer. . Notes: . We use Adam as our optimizer and using accuracy and f1 as our evaluating metrics, just like in the original paper. . class TDLSTM(pl.LightningModule): def __init__(self, embeddings, hidden_size, num_layers=1, num_classes=3, batch_first=True, lr=1e-3, dropout=0, l2reg=0.01): super().__init__() embedding_dim = embeddings.shape[1] self.embedding = nn.Embedding.from_pretrained(embeddings) # load pre-trained word embeddings self.l_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.r_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=batch_first, dropout=dropout) self.linear = nn.Linear(hidden_size*2, num_classes) self.lr = lr self.l2reg = l2reg # Define metrics self.train_acc = torchmetrics.Accuracy() self.val_acc = torchmetrics.Accuracy() self.val_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) self.test_acc = torchmetrics.Accuracy() self.test_f1 = torchmetrics.F1(num_classes=3, average=&#39;macro&#39;) def configure_optimizers(self): optim = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.l2reg) return optim def forward(self, padded_l_seqs, l_lens, padded_r_seqs, r_lens): # convert seq to word vector padded_l_embeds = self.embedding(padded_l_seqs) padded_r_embeds = self.embedding(padded_r_seqs) # pack the embeds padded_l_seq_pack = pack_padded_sequence(padded_l_embeds, l_lens, batch_first=True, enforce_sorted=False) padded_r_seq_pack = pack_padded_sequence(padded_r_embeds, r_lens, batch_first=True, enforce_sorted=False) _, (h_l, _) = self.l_lstm(padded_l_seq_pack) _, (h_r, _) = self.r_lstm(padded_r_seq_pack) h = torch.cat((h_l[-1], h_r[-1]), -1) # B x 2H out = self.linear(h) return out def training_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.train_acc(scores, sentiments) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;train_acc&#39;, self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) return loss def validation_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) loss = F.cross_entropy(logits, sentiments) scores = F.softmax(logits, dim=-1) self.val_acc(scores, sentiments) self.val_f1(scores, sentiments) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) self.log(&#39;val_acc&#39;, self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True) self.log(&#39;val_f1&#39;, self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True) def test_step(self, batch, batch_idx): # pylint: disable=unused-argument (padded_l_seqs, l_lens), (padded_r_seqs, r_lens), sentiments = batch logits = self.forward(padded_l_seqs, l_lens, padded_r_seqs, r_lens) scores = F.softmax(logits, dim=-1) self.test_acc(scores, sentiments) self.test_f1(scores, sentiments) self.log(&#39;test_acc&#39;, self.test_acc, on_step=False, on_epoch=True, logger=True) self.log(&#39;test_f1&#39;, self.test_f1, on_step=False, on_epoch=True, logger=True) . Training . checkpoint_callback = ModelCheckpoint( monitor=&#39;val_acc&#39;, # save the model with the best validation accuracy dirpath=&#39;checkpoints&#39;, filename=&#39;best_model&#39;, mode=&#39;max&#39;, ) tb_logger = pl_loggers.TensorBoardLogger(&#39;logs/&#39;) # create logger for tensorboard # hyper-parameters lr = 1e-3 hidden_size = 500 num_epochs = 60 l2reg = 0.5 trainer = pl.Trainer(gpus=1, max_epochs=num_epochs, logger=tb_logger, callbacks=[checkpoint_callback]) model = TDLSTM(embeddings, hidden_size, lr=lr, l2reg=l2reg) trainer.fit(model, dataloaders, test_dataloaders) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | embedding | Embedding | 119 M 1 | l_lstm | LSTM | 1.2 M 2 | r_lstm | LSTM | 1.2 M 3 | linear | Linear | 3.0 K 4 | train_acc | Accuracy | 0 5 | val_acc | Accuracy | 0 6 | val_f1 | F1 | 0 7 | test_acc | Accuracy | 0 8 | test_f1 | F1 | 0 - 2.4 M Trainable params 119 M Non-trainable params 121 M Total params 487.050 Total estimated model params size (MB) . . new_model = TDLSTM.load_from_checkpoint(checkpoint_callback.best_model_path, embeddings=embeddings, hidden_size=500) trainer.test(new_model, test_dataloaders) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_acc&#39;: 0.7037572264671326, &#39;test_f1&#39;: 0.6847572326660156} -- . [{&#39;test_acc&#39;: 0.7037572264671326, &#39;test_f1&#39;: 0.6847572326660156}] . from IPython.display import Image Image(filename=&#39;images/results.png&#39;) . Compare to the result from the paper, our implementation gets very close results. You can try to tune the model to get better result. .",
            "url": "https://minhdang241.github.io/minhdg-blog/implementation/2020/01/28/NLP_1_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-1.html",
            "relUrl": "/implementation/2020/01/28/NLP_1_Effective_LSTMs_for_Target_Dependent_Sentiment_Classification-Part-1.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://minhdang241.github.io/minhdg-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, Im Minh. Welcome to my NLP365 project. This is where I document all the things I have learned and researched about NLP and ML in general. .",
          "url": "https://minhdang241.github.io/minhdg-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://minhdang241.github.io/minhdg-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}