{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: Word2Vec [Part1]\n",
    "> Implement Word2Vec using Pytorch and Gensim\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [implementation]\n",
    "- image: images/word2vec.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full notebook is available [here](https://github.com/minhdang241/minhdg-blog/blob/master/_notebooks/2021-07-26-NLP_6_Word_Embeddings_(Word2Vec)_%5BPart_1%5D.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EovZw18Krm29"
   },
   "source": [
    "# Word Embeddings Overview\n",
    "\n",
    "Word embeddings are dense vectors of real numbers, one for each word in the vocabulary, which is the collection of words extracted from the dataset. \n",
    "\n",
    "There are many ways to represent a word on a computer. For example, we can use ASCII code. Yet, it only tells what the word is, not its meaning. Another option is to use a one-hot vector to represent a word, in which we put the number 1 in the location of the represented word. However, using a one-hot vector has 2 main drawbacks. First of all, the vector is huge and sparse. The size of the vector is the same as the size of the vocabulary. The vector is sparse since there is only one position that has a non-zero value. Next, it treats all words independently, with no relation to each other. Technically said it does not provide any notion of *similarity* between words.\n",
    "\n",
    "Take an example from Pytorch documentation:\n",
    "\n",
    "> Suppose we are building a language model. Suppose we have seen the sentences.\n",
    "* The mathematician ran to the store.\n",
    "* The physicist ran to the store.\n",
    "* The mathematician solved the open problem.\n",
    ">\n",
    ">In the training data, suppose we get the sentence never seen before:\n",
    "* The physicist solved the open problem.\n",
    "Our language model might be doing OK on this sentence. But it's better if we can use the following facts:\n",
    "1. We have seen the mathematician and physicist in the same role in the sentence. As a result, they can have a semantic relation somehow.\n",
    "2. We have seen mathematicians in the same role in this new unseen sentence as we are now seeing physicists.\n",
    ">\n",
    ">That infers the physicist is a good fit in the new unseen sentence. That's what we mean by *semantic similarity*. That relies on the assumption that: ***words appearing in similar contexts are related to each other semantically*** \n",
    "\n",
    "To encode the similarity between words we can think up some semantic attributes. We then put those attributes in the vector and give a score for each. We give high scores for shared attributes and low scores for the counterparts. We can measure the similarity between two word vectors using the dot product. As a result, similar words will have a similarity score near 1 and different words will have a similarity score near zero.\n",
    "\n",
    "Since thinking of the semantic attributes is hard and manually intensive, we can let them be the parameters in the network and be updated during training. One drawback of doing this way is the attribute scores are not interpretable. That is, we do not know what is attribute that a specific score represents.\n",
    "\n",
    "In summary, **word embeddings are a representation of the semantics of a word, efficiently encoding semantic information that might be relevant to the task at hand**\n",
    "\n",
    "To read more about word embeddings in Pytorch, click [here](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPGG6TaRCU2I"
   },
   "source": [
    "# Word2Vec\n",
    "Word2Vec is one the approaches to develop a word embedding. There are two algorithms used in Word2Vec: **continuous bag-of-words** (CBOW) and **skip-gram**. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the probability of context words from a center word. In this post, we will try to implement both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxxzbWOg2-MA"
   },
   "source": [
    "# Continuous Bag-of-words (CBOW) (Pytorch)\n",
    "\n",
    "The algorithm aims to predict a center word give the surrounding context in terms of word vectors. For example, given a sentence \"The cat jumped over the puddle\", the algorithm treats {\"The\", \"cat\", \"over\", \"the\", \"puddle\"} as context words and {\"jumped\"} as the center word. The objective is to generate the center word from context words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPnyWqZC3PRb",
    "outputId": "25c43019-bac8-485c-d0af-ff13be7a7e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff4203ac150>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# reproduction purpose\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjWOZb0p28ii",
    "outputId": "b2014af5-5ec2-4b6c-f1bd-c1eda18819f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[238.0529305934906, 233.41328835487366, 228.94981503486633, 224.64973831176758, 220.50258708000183, 216.49783158302307, 212.62398993968964, 208.87176704406738, 205.23141360282898, 201.69729340076447]\n",
      "tensor([-0.7098, -0.6179, -0.3807,  2.3069, -0.7957,  1.4458,  0.6856,  2.1891,\n",
      "        -0.2936,  0.5549], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 128\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        out = torch.sum(out, dim=0).view(1, -1)\n",
    "        out = F.relu(self.linear1(out))        \n",
    "        out = F.log_softmax(self.linear2(out), dim=-1)\n",
    "        return out\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_idxs = make_context_vector(context, word_to_ix)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)\n",
    "print(model.embeddings.weight[word_to_ix['spirits']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQGIQp3OEzDt"
   },
   "source": [
    "# Skip-gram (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qO6Ttx5iEyJZ",
    "outputId": "3cf041bf-a1d3-486c-bd35-b400d73a2f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[912.227198600769, 903.7498636245728, 895.8144631385803, 888.3975474834442, 881.4723126888275, 875.0154674053192, 869.0011675357819, 863.3996088504791, 858.1725707054138, 853.288257598877]\n",
      "tensor([ 0.3021,  0.2816, -1.1773,  1.0418,  1.8390, -0.5845, -0.2637,  1.3842,\n",
      "         0.3855,  0.1923], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 128\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    for value in context:\n",
    "        sample = (target, value)\n",
    "        data.append(sample)\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        out = F.relu(self.linear1(out))        \n",
    "        out = F.log_softmax(self.linear2(out), dim=-1)\n",
    "        return out\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGram(vocab_size, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for input, output in data:\n",
    "        input_index = torch.tensor([word_to_ix[input]], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(input_index)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[output]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)\n",
    "print(model.embeddings.weight[word_to_ix['spirits']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKmCMucUelzq"
   },
   "source": [
    "# Word2Vec (Gensim)\n",
    "Gensim implements CBOW and using negative sampling for training by default. To toggle between CBOW and skip-gram algorithm, add this argument below when create the Word2Vec instance. <br>\n",
    "`sg ({0, 1}, optional) â€“ Training algorithm: 1 for skip-gram; otherwise CBOW.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4493uhASiWSu"
   },
   "outputs": [],
   "source": [
    "#Install the lastest version of gensim 4.0.0\n",
    "%%capture\n",
    "!pip install --upgrade gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zIpTudLhwXV"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences\"\"\"\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there is one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfrzkNb9jDEy"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, sg=0) # change to 1 if prefer skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34FxF4Guplus",
    "outputId": "909e2380-ff2d-4944-ba7a-941e61ba9454"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.47548895e-02,  4.44000289e-02,  1.02321925e-02,  1.20065575e-02,\n",
       "        9.83571820e-03, -8.47978592e-02,  3.42624560e-02,  8.44758376e-02,\n",
       "       -3.13533121e-03, -1.38494289e-02, -4.28904686e-03, -5.30756600e-02,\n",
       "        7.55382003e-03,  2.79652104e-02,  4.44820989e-03,  1.32240532e-02,\n",
       "       -2.42202985e-03, -2.49751448e-03, -1.71462744e-02, -6.11230545e-02,\n",
       "        3.83632220e-02,  9.09661502e-03,  1.09449634e-02, -2.17360468e-03,\n",
       "       -1.88374687e-02,  2.02645455e-02, -1.86126940e-02, -1.27745485e-02,\n",
       "       -2.71721575e-02,  1.31690372e-02,  3.29722501e-02, -4.22514454e-02,\n",
       "        3.72793637e-02, -3.36719528e-02, -7.06554204e-03,  4.73929197e-02,\n",
       "        1.39981424e-02,  7.61039788e-03, -1.61971990e-02, -3.04519087e-02,\n",
       "       -1.60803776e-02,  4.38297074e-03, -8.02283920e-03,  1.50885303e-02,\n",
       "        2.63876691e-02, -1.95540637e-02, -2.64777783e-02, -3.67977191e-04,\n",
       "        7.01137306e-03,  3.12562287e-02,  1.64159592e-02, -2.16274485e-02,\n",
       "       -1.62629951e-02,  8.53445439e-04, -1.33869080e-02,  1.73475724e-02,\n",
       "       -1.21692673e-03,  2.21166899e-03, -2.24457402e-02,  4.26836731e-03,\n",
       "       -1.45576373e-02,  6.20996347e-04,  6.98805647e-03, -4.57839714e-03,\n",
       "       -2.95367688e-02,  6.10822700e-02,  1.47746662e-02,  3.35532837e-02,\n",
       "       -3.87191810e-02,  4.92215976e-02, -1.04450071e-02,  2.97265081e-03,\n",
       "        5.04135974e-02, -8.13318323e-03,  3.63118313e-02,  2.79957112e-02,\n",
       "       -1.12850778e-03, -2.14369707e-02, -4.13609855e-02, -1.58206820e-02,\n",
       "       -3.22486572e-02,  7.98239373e-03, -3.16767953e-02,  4.03956585e-02,\n",
       "       -3.79999110e-05, -1.51074128e-02,  2.10159868e-02,  3.33536156e-02,\n",
       "        4.75050472e-02,  1.45110274e-02,  3.53002362e-02,  5.23244813e-02,\n",
       "        4.45592292e-02,  9.90339927e-03,  8.80143940e-02,  2.01153327e-02,\n",
       "        4.54641357e-02, -4.78953496e-03,  7.65400566e-03, -5.82322525e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voR1UeQGswLc"
   },
   "source": [
    "Word2Vec is unsupervised task, so there is no good way to evaluate the result. Evaluation depends on the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsYFE8_8FOoP"
   },
   "source": [
    "## Example of Gensim Word2Vec functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_5kZ-DLfrkh",
    "outputId": "04e20fde-f4d2-4688-d459-41f8d4a9be99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.8% 1660.2/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsBWLPUVhiq-",
    "outputId": "9f5c5f16-d946-4164-d3c9-58abc44c6508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index , word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKeWj9b1m8We"
   },
   "outputs": [],
   "source": [
    "vec_queen = wv['queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvH4WyBFh4RB"
   },
   "source": [
    "One limitation of Word2Vec is that the model is unable to infer vectors for unseen words.\n",
    "\n",
    "Note: FastText model can solve this limitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9_J6wPrnJJz",
    "outputId": "fb551fe4-77d5-487b-ea6b-8653fd8201fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'vietname' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_random = wv['vietname']\n",
    "except KeyError:\n",
    "    print(\"The word 'vietname' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKV0Uvd6n_4T",
    "outputId": "619a1a13-71c6-4149-b612-800eb82e317e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scooter\tchair\t0.20833881199359894\n",
      "scooter\tmotorbike\t0.7071131467819214\n",
      "scooter\tfootball\t0.07120829075574875\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "         ('scooter', 'chair'),\n",
    "         ('scooter', 'motorbike'),\n",
    "         ('scooter', 'football')\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print(f'{w1}\\t{w2}\\t{wv.similarity(w1, w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWzRmK_Noo-F",
    "outputId": "e8ad6c7d-b7f4-4c10-f8d2-5d445845dc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ww2', 0.6164373159408569), ('iraq', 0.6033741235733032), ('reagan', 0.5772603154182434), ('VietNam', 0.5732988119125366), ('afghanistan', 0.5602078437805176)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['vietnam'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um_GyGXqoyme",
    "outputId": "d016ab05-c1e6-49ed-afec-c079417ec169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['you', \"don't\", \"know\", \"me\", \"son\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdCQIkVfyF2M"
   },
   "source": [
    "# Visualize the Word Embeddings using tSNE\n",
    "\n",
    "Visualization can be used to notice semantic and syntactic trends in the data.\n",
    "\n",
    "* Semantic: words like cat, dog, cow have a tendency to lie close by.\n",
    "* Syntactic: words like run, running or cut, cutting lie close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me4p8jm5_l99"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fSCJHwTxgZ2"
   },
   "source": [
    "# Using Pretrained Word2Vec in Pytorch\n",
    "1. Construct the vocabulary of our own data.\n",
    "2. Load word vectors corresponding to words in our vocabulary.\n",
    "3. Use the our word2index to translate our text to indices."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP 6: Word Embeddings (Word2Vec) [Part 1].ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
