{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job recommendation system for CS/IT program.\n",
    "> Implement job recommendation system for CS/IT program\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [implementation]\n",
    "- image: images/cs_it_recommender.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx7m0rAvSIq6"
   },
   "source": [
    "# Overview:  \n",
    "\n",
    "Taxonomy of recommender systems: A framework to classify and analyze a particular recommendation system.The system is described by the following dimensions:\n",
    "\n",
    "    1. domain\n",
    "    2. purpose\n",
    "    3. context\n",
    "    4. personalize level\n",
    "    5. whose opinions\n",
    "    6. privacy and trustworthiness\n",
    "    7. interfaces\n",
    "    8. algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxANcU5SS2Kh"
   },
   "source": [
    "**Domain**: Type of content recommended. The domain of Netflix is movies and TV series.\n",
    "\n",
    "**Purpose**: What is the purpose of the system, both for the end user and for the provider?\n",
    "\n",
    "**Context**: The environment which the consumer recieves a recommendation.\n",
    "\n",
    "**Personalization Levels**:\n",
    "1. None personalized\n",
    "2. Semi/Segment personalized\n",
    "3. Personalized recommendation is based on data about the current user than indicates how the user has interacted with the system previously.\n",
    "\n",
    "**Algorithms**:\n",
    "Content-based filtering uses the metadata having on the items in the catalog. Depending on the specific algorithm, the system can calculate recommendations either by taking the items the user has liked and finding similar content, by comparing the items and user profiles, or if there's no user involved, by finding similar content between items. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJVYzDu9wdzY"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oszpn8xgNJEw",
    "outputId": "a55af77d-4af2-4777-ab0f-4f8c26a92438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRVgfOpBnAcB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sm7Ht6sgoRT9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install beautifulsoup4\n",
    "!python -m spacy download en_core_web_md\n",
    "!pip install -U sentence-transformers\n",
    "!pip install Unidecode\n",
    "!pip install symspellpy\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itlHVGyGoLsJ"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9SY9a4IoEdc"
   },
   "outputs": [],
   "source": [
    "course_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Text Classification/datasets/job_recommendation/dataset.csv\")\n",
    "job_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Text Classification/datasets/job_recommendation/job_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neHwsLG9VEd5"
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GSRcF2Ou9IR"
   },
   "outputs": [],
   "source": [
    "# Change the dataframe here to preprocess course_df and job_df\n",
    "# df = job_df\n",
    "df = course_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT_fgHVKjtet"
   },
   "outputs": [],
   "source": [
    "# Remove known NA course in the course_df\n",
    "# df = course_df\n",
    "# df.drop([67], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsVey1LZEY5Z"
   },
   "outputs": [],
   "source": [
    "# For job_df create a new column to store description\n",
    "# df['long_description'] =df['description'] \n",
    "# df['description'] = df['short_description']\n",
    "# df = df.dropna(axis=0, subset=['description', 'long_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hPlRCinhO-b"
   },
   "source": [
    "## Step 1: Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1p0WBNhhUVw"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BKBh5wncFGD"
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vn3uyLLfhfAg",
    "outputId": "a645eb1d-5b19-429e-b098-ade23179a23c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['no_html'] = df['description'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8i7WumjNBH"
   },
   "source": [
    "## Step 2: Add space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMghr_ILmp2L"
   },
   "source": [
    "### 2.1 Normalize space rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4f8v_nPi5ao"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def add_spaces(text):\n",
    "    # Space after punc. Only apply to !.,;:? \n",
    "    # Eg: Dislike.However => Dislike. However \n",
    "    text = re.compile(r\"([!.,;:?])([A-Z])\").sub(r\"\\1 \\2\", text)\n",
    "    # Space before open bracket\n",
    "    # Eg: Dislike(sth) => Dislike (sth)\n",
    "    text = re.sub(r\"([A-za-z])([\\(\\{\\[])\", r'\\1 \\2', text)\n",
    "    # Space after close bracket\n",
    "    # Eg: (such as)I like => (such as) I like\n",
    "    text = re.sub(r\"([\\)\\}\\]])([A-Za-z])\", r'\\1 \\2', text)\n",
    "    # Space between word and - or +\n",
    "    # Eg: I like it because-fast -pretty => I like it because - fast - pretty\n",
    "    # Eg: I like mac-book => keep the same\n",
    "    text = re.sub(r\"([A-Za-z])([-+])(\\s)\", r'\\1 \\2 ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9Qt_vwrjvCu",
    "outputId": "102fd313-3ab6-4eb4-8f9e-6528ec829418"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['add_space'] = df['no_html'].apply(add_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "MsH2bXo4l5M1",
    "outputId": "b273be3b-d219-4b9a-b164-ece99efb8c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impacted row 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_html</th>\n",
       "      <th>add_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Experience in Agile. development methodologies. Integration of user-facing elements developed by a back-end developer with server- side logic</td>\n",
       "      <td>Experience in Agile. development methodologies. Integration of user-facing elements developed by a back-end developer with server - side logic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          no_html  \\\n",
       "53  Experience in Agile. development methodologies. Integration of user-facing elements developed by a back-end developer with server- side logic   \n",
       "\n",
       "                                                                                                                                         add_space  \n",
       "53  Experience in Agile. development methodologies. Integration of user-facing elements developed by a back-end developer with server - side logic  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df[df.no_html != df.add_space][[\"no_html\", \"add_space\"]]\n",
    "print(\"Impacted row\", len(temp_df))\n",
    "temp_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGM7Kbe-mvCc"
   },
   "source": [
    "### 2.2 Add space and remove listing number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FX9KQSMJmOTK"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def handle_listing_number(text):\n",
    "    \"\"\"\n",
    "    Only remove if it is listing numbers such as 1. 2. 3.\n",
    "    \"\"\"\n",
    "    listing = []\n",
    "    def remove_listing_number(match_obj):\n",
    "        listing_number = match_obj.group(0).strip()\n",
    "        result = \"\"\n",
    "        if match_obj.start() == 0 and listing_number[0] == \"1\":\n",
    "            return \"\"\n",
    "        for c in listing_number:\n",
    "            if c.isdigit():\n",
    "                # save to listing if it is digit\n",
    "                listing.append(int(c))\n",
    "                break\n",
    "            result += c\n",
    "        if not any(c in string.punctuation for c in result):\n",
    "            result += \".\"\n",
    "        return result + \" \"\n",
    "    \n",
    "    new_text = re.sub(r\"\\b([\\.\\;\\:\\!\\?\\D]*)([1-9])\\.\\s\", remove_listing_number, text.strip())\n",
    "    if len(listing) > 1:\n",
    "        listing_copy = listing[:]\n",
    "        listing_copy.sort()\n",
    "        # if listing is sorted\n",
    "        if listing_copy == listing:\n",
    "            return new_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0n71AvS5m0XL",
    "outputId": "f2d2d174-aa56-4bbc-c58e-201f0d86660c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"add_space2\"] = df['add_space'].apply(handle_listing_number) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "AMlVsaq4m0uA",
    "outputId": "a4749759-6ad6-4a07-9e1f-4383119cd22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impacted row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add_space</th>\n",
       "      <th>add_space2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [add_space, add_space2]\n",
       "Index: []"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df[df.add_space2 != df.add_space][[\"add_space\", \"add_space2\"]]\n",
    "print(\"Impacted row\", len(temp_df))\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLKZ4eO5nVGw"
   },
   "source": [
    "## Step 3: Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYZDzfW9nY5C"
   },
   "outputs": [],
   "source": [
    "from symspellpy import SymSpell, Verbosity, helpers\n",
    "import contractions\n",
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    expand shortened words, e.g. don't to do not\n",
    "    contractions library does not keep character case => need to transfer casing from origin text to fixed text\n",
    "    \"\"\"\n",
    "    expanded_text = helpers.transfer_casing_for_similar_text(text, contractions.fix(text))\n",
    "    # uppercase I\n",
    "    return re.sub(r\"\\bi\\b\", \"I\", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWLI7xqYnZJH",
    "outputId": "1e43a98b-0740-4d19-e233-8fc1783fb03c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"expand_contractions\"] = df[\"add_space2\"].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXlxWvCFoD5I"
   },
   "source": [
    "## Step 4: Remove redundant elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBUa1r6XnrGP"
   },
   "outputs": [],
   "source": [
    "# Remove and replace by empty space\n",
    "def remove_redundant_elements(text):\n",
    "    # remove urls\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    # remove phone\n",
    "    text = re.sub(r\"[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}\", \" \", text)\n",
    "    # remove email\n",
    "    text = re.sub(r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\", \" \", text)\n",
    "    # remove newline\n",
    "    table = str.maketrans(\"\\n\\t\\r\", \"   \")\n",
    "    text = text.translate(table)\n",
    "    # remove redundant whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnLNlY4EoUrU",
    "outputId": "0fcba542-54cb-4585-bdc2-f5367ec7af30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"no_redundant\"] = df[\"expand_contractions\"].apply(remove_redundant_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGnO-bndoZRr"
   },
   "source": [
    "## Step 5: Remove emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm_of-dmorwC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDWPR2yBoYd4"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(r\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJ02gq2loU9n",
    "outputId": "714c0fe3-5ee2-43be-d5c7-b919a38e8c89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"no_emoji\"] = df[\"no_redundant\"].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLFyeydYpJq4"
   },
   "source": [
    "## Step 6: Removing accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ1v1bOFopek"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JaG7u__pNCc",
    "outputId": "1e84964e-026c-4651-fd15-788202f621cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"no_accented\"] = df[\"no_emoji\"].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stp7x0Nxpbfl"
   },
   "source": [
    "## Step 7: Collapse duplicated punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0vVJencpYid"
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "def collapse_duplicated_punctuations(text):\n",
    "    \"\"\"\n",
    "    collapse duplicated punctations\n",
    "    because we added space to separate punc and word in step 3, no need to append \" \" after punc\n",
    "    \"\"\"\n",
    "    newtext = []\n",
    "    for k, g in groupby(text):\n",
    "        if k in string.punctuation:\n",
    "            newtext.append(k)\n",
    "        else:\n",
    "            newtext.extend(g)\n",
    "\n",
    "    return ''.join(newtext) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_av0F4GgpiPv",
    "outputId": "32eac22f-e3ec-4c73-8f56-9a273db58abb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"no_duplicated_punc\"] = df[\"no_accented\"].apply(collapse_duplicated_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty96sEJHpp5z"
   },
   "source": [
    "## Step 8: Remove consecutive spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnUieWhTpijM",
    "outputId": "fe9101e9-b266-483f-b11b-45e000761523"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"no_consecutive_spaces\"] = df.no_duplicated_punc.replace({\"\\s+\":\" \"},regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2Qe1si4puD8",
    "outputId": "74580c1f-e1b9-45b3-9cf8-ac61cd995c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# remove redundant empty space \n",
    "df[\"no_consecutive_spaces\"] = df.no_consecutive_spaces.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7NS5gyp-eF"
   },
   "source": [
    "## Step 9: Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJYPOaYBpvpN",
    "outputId": "2ab85c41-edc2-4d42-ae4b-54eaedf83fa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['lower_case'] = df[\"no_consecutive_spaces\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "QnIJ7Ym2qEoA",
    "outputId": "44f1c7fc-04d0-4ccb-ec98-3b4b637a44d4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'participate in all. development activities. write high-quality code to implement. features or fix bugs and implement unit test'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower_case'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqOJTgqayjof"
   },
   "source": [
    "## Step 9: Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BoN-i4trfdT"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqBKpqXpyid_"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    doc = nlp(text)\n",
    "    no_stop_words = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            no_stop_words.append(token.text)\n",
    "    return ' '.join(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raVD22huzhXi",
    "outputId": "ed5cd3fa-9fe2-4483-bcc4-973694ff6244"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['no_sw'] = df['lower_case'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0H3Z2iT4zm_D",
    "outputId": "46e611c6-4a18-4342-8ec5-c591411a1ca7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'chance work talented developers , following high standard . development practices ci / cd processes'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['no_sw'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTu5moLCkrog"
   },
   "source": [
    "# Load preprocessed datasets (If it is availabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NSUiqpSkvCq"
   },
   "outputs": [],
   "source": [
    "job_df = pd.read_csv(\"job_df.csv\")\n",
    "course_df = pd.read_csv(\"course_df.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD_8YOX6qSXJ"
   },
   "source": [
    "# Implement the Recommender \n",
    "\n",
    "We will develop the recommender, which is used to suggest jobs based on learned courses. \n",
    "\n",
    "To recommend the jobs based on courses we leverage 2 models via 2 steps: \n",
    "\n",
    "**Step 1:** Firstly, we use sentence transformer model to calculate the vector embeddings for chosen courses and jobs. Then we compare the the similarity score (cosine score) between them and filter the top 10.\n",
    "\n",
    "**Step 2:** Secondly, we use the NER model to extract the skills from chosen courses and courses. Then we calculate the score for each job by counting the matching skills with chosen courses. That score will be used to sort the jobs to produce the final order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4R5tO-HwLo2"
   },
   "source": [
    "## Load the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Gqw8gjJ0sFH"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"mrm8488/codebert-base-finetuned-stackoverflow-ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIV7ZpXQqGv4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39MBamIt0UsE"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqG67kBfwovr"
   },
   "source": [
    "## Load the sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7qPimB0BDv-"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sent_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRzcf4J2xEz8"
   },
   "source": [
    "## Define helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0-s6FEFtetO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "def add_space(ent):\n",
    "    ent['word'] = ent['word'].replace('Ġ', ' ')\n",
    "    return ent\n",
    "\n",
    "def merge_B_I_entities(ents):\n",
    "    results = []\n",
    "    i = 0\n",
    "    N = len(ents)\n",
    "    while i < N:\n",
    "        ent = ents[i]\n",
    "        ent = add_space(ent)\n",
    "        if i < N - 1 and ent['entity'][:2] == 'B-':\n",
    "            i += 1\n",
    "            next_ent = ents[i]\n",
    "            while i < N and next_ent['entity'][:2] == 'I-':\n",
    "                ent['word'] += add_space(next_ent)['word']\n",
    "                i += 1 \n",
    "                if i < N:\n",
    "                    next_ent = ents[i]\n",
    "                else:\n",
    "                    break\n",
    "            i -= 1\n",
    "            ent['end'] = ents[i]['end']\n",
    "            ent['word'] = ent['word'].strip().lower()\n",
    "        results.append(ent)\n",
    "        i += 1\n",
    "    return results \n",
    "\n",
    "def merge_entity(ent1, ent2):\n",
    "    if ent1['end'] == ent2['start']:\n",
    "        ent = {'start': ent1['start'], 'end': ent2['end'], 'entity': ent1['entity'], 'word': (ent1['word']+ent2['word']).strip().lower()}\n",
    "        return ent\n",
    "\n",
    "def merge_similar_entities(ents):\n",
    "    results = []\n",
    "    hash_map = defaultdict(list)\n",
    "    for ent in ents:\n",
    "        hash_map[ent['entity']].append(ent)\n",
    "    for k, v in hash_map.items():\n",
    "        new_ents = []\n",
    "        merge_ent = v[0]\n",
    "        i = 0\n",
    "        while i < len(v) - 1:\n",
    "            temp = merge_entity(merge_ent, v[i + 1])\n",
    "            if temp:\n",
    "                merge_ent = temp\n",
    "            else:\n",
    "                new_ents.append(merge_ent)\n",
    "                merge_ent = v[i + 1]\n",
    "            i+=1\n",
    "        merge_ent['word'] = merge_ent['word'].strip().lower()\n",
    "        new_ents.append(merge_ent)\n",
    "        results += new_ents \n",
    "    words = [ent['word'] for ent in results]\n",
    "    return results\n",
    "\n",
    "def extract_skills(desc):\n",
    "    ents = classifier(desc)\n",
    "    results = merge_B_I_entities(ents)\n",
    "    results = merge_similar_entities(results)\n",
    "    skills = set()\n",
    "    for ent in results:\n",
    "        skills.add(ent['word'])\n",
    "    return list(skills)\n",
    "\n",
    "def get_skills_from_course_titles(course_titles):\n",
    "    course_skills = set()\n",
    "    for skills in course_df[course_df['title'].isin(course_titles)]['skills']:\n",
    "        course_skills.update(ast.literal_eval(skills))\n",
    "    return course_skills\n",
    "\n",
    "def show_results(results):\n",
    "    for res in results:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd68koO6xJEK"
   },
   "source": [
    "## Define Recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0MhMb6d6BMy"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import ast\n",
    "import torch\n",
    "def get_skills_from_course_titles(course_titles):\n",
    "    course_skills = set()\n",
    "    for skills in course_df[course_df['title'].isin(course_titles)]['skills']:\n",
    "        course_skills.update(ast.literal_eval(skills))\n",
    "    return course_skills\n",
    "\n",
    "class Recommender:\n",
    "    def __init__(self, ner_classifier, sent_model, course_df, setup=True):\n",
    "        self.classifier = ner_classifier\n",
    "        self.sent_model = sent_model\n",
    "        self.course_info = course_df\n",
    "        if setup:\n",
    "            self._setup(course_df)\n",
    "\n",
    "    def _setup(self, course_df):\n",
    "        self.course_info = course_df.copy(deep=True)\n",
    "        self.course_info['skills'] = self.course_info['extract_skills'].apply(self.extract_skills)\n",
    "\n",
    "    def extract_skills(self, desc):\n",
    "        ents = self.classifier(desc)\n",
    "        results = Recommender.merge_B_I_entities(ents)\n",
    "        results = Recommender.merge_similar_entities(results)\n",
    "        skills = set()\n",
    "        for ent in results:\n",
    "            skills.add(ent['word'])\n",
    "        return list(skills) \n",
    "\n",
    "    def recommend(self, course_titles, job_info, topk: int = None) -> List[Job]:\n",
    "        c_embed = self.sent_model.encode(list(self.course_info[self.course_info['title'].isin(course_titles)]['no_sw']), convert_to_tensor=True)\n",
    "        c_avg_embed = torch.mean(c_embed, axis=0)\n",
    "        job_descs = list(job_info['no_sw'])\n",
    "        job_embeds = self.sent_model.encode(job_descs, convert_to_tensor=True)\n",
    "\n",
    "        results = Recommender.compare_embeds(c_avg_embed, job_embeds, job_info)\n",
    "        skill_list = [set(res[2]) for res in results]\n",
    "        my_skills = Recommender.get_skills_from_course_titles(course_titles)\n",
    "        scores = []\n",
    "\n",
    "        # calculate similar skills scores\n",
    "        for skills in skill_list:\n",
    "            mutual = my_skills.intersection(skills)\n",
    "            if len(my_skills) != 0:\n",
    "                scores.append(len(mutual)/len(my_skills))\n",
    "            else:\n",
    "                scores.append(0)\n",
    "\n",
    "        for i, res in enumerate(results):\n",
    "            final_score = res[-1] * 0.6 + 0.4 * scores[i]\n",
    "            res[-1] = final_score\n",
    "\n",
    "        results.sort(key=lambda x: x[-1], reverse=True)\n",
    "        if topk:\n",
    "            results = results[:topk]\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def get_skills_from_course_titles(course_titles):\n",
    "        course_skills = set()\n",
    "        for skills in course_df[course_df['title'].isin(course_titles)]['skills']:\n",
    "            course_skills.update(ast.literal_eval(skills))\n",
    "        return course_skills\n",
    "\n",
    "    @staticmethod\n",
    "    def add_space(ent):\n",
    "        ent['word'] = ent['word'].replace('Ġ', ' ')\n",
    "        return ent\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_B_I_entities(ents):\n",
    "        results = []\n",
    "        i = 0\n",
    "        N = len(ents)\n",
    "        while i < N:\n",
    "            ent = ents[i]\n",
    "            ent = add_space(ent)\n",
    "            if i < N - 1 and ent['entity'][:2] == 'B-':\n",
    "                i += 1\n",
    "                next_ent = ents[i]\n",
    "                while i < N and next_ent['entity'][:2] == 'I-':\n",
    "                    ent['word'] += add_space(next_ent)['word']\n",
    "                    i += 1 \n",
    "                    if i < N:\n",
    "                        next_ent = ents[i]\n",
    "                    else:\n",
    "                        break\n",
    "                i -= 1\n",
    "                ent['end'] = ents[i]['end']\n",
    "                ent['word'] = ent['word'].strip().lower()\n",
    "            results.append(ent)\n",
    "            i += 1\n",
    "        return results \n",
    "\n",
    "    @staticmethod\n",
    "    def merge_entity(ent1, ent2):\n",
    "        if ent1['end'] == ent2['start']:\n",
    "            ent = {'start': ent1['start'], 'end': ent2['end'], 'entity': ent1['entity'], 'word': (ent1['word']+ent2['word']).strip().lower()}\n",
    "            return ent\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_similar_entities(ents):\n",
    "        results = []\n",
    "        hash_map = defaultdict(list)\n",
    "        for ent in ents:\n",
    "            hash_map[ent['entity']].append(ent)\n",
    "        for k, v in hash_map.items():\n",
    "            new_ents = []\n",
    "            merge_ent = v[0]\n",
    "            i = 0\n",
    "            while i < len(v) - 1:\n",
    "                temp = Recommender.merge_entity(merge_ent, v[i + 1])\n",
    "                if temp:\n",
    "                    merge_ent = temp\n",
    "                else:\n",
    "                    new_ents.append(merge_ent)\n",
    "                    merge_ent = v[i + 1]\n",
    "                i+=1\n",
    "            merge_ent['word'] = merge_ent['word'].strip().lower()\n",
    "            new_ents.append(merge_ent)\n",
    "            results += new_ents \n",
    "        words = [ent['word'] for ent in results]\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_embeds(c_embed: List, job_embeds: List[List], job_info):\n",
    "        results = []\n",
    "        for i, job_embed in enumerate(job_embeds):\n",
    "            score = util.pytorch_cos_sim(c_embed, job_embed)\n",
    "            results.append([job_info.index[i], job_info['title'].iloc[i], job_info['skills'].iloc[i], score.item()])\n",
    "        results.sort(key=lambda x: x[-1], reverse=True)\n",
    "        return results\n",
    "\n",
    "    @staticmethod  \n",
    "    def post_process(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keT11Q3kxN6t"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By42oXJXHDWk"
   },
   "outputs": [],
   "source": [
    "## Preprocessing: Extract CS/IT skills in courses and job descriptions\n",
    "course_df['skills'] = course_df['description'].apply(extract_skills)\n",
    "job_df['skills'] = job_df['long_description'].apply(extract_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEmINWTs-0xu"
   },
   "outputs": [],
   "source": [
    "my_recommender = Recommender(classifier, sent_model, course_df, setup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8auYrtab7MEM"
   },
   "outputs": [],
   "source": [
    "course_titles = ['Artificial Intelligence', 'Machine Learning', 'Mathematics for Computing 2', 'Foundations of Artificial Intelligence for STEM']\n",
    "results = my_recommender.recommend(course_titles, job_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J084xa2OEtyS",
    "outputId": "8c24d480-af89-4214-b5a3-8a05f73b9d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 'Software Engineer in Data Science', \"['google deepmind', 'autopilot', 'autopilot ai', 'vinai', 'python', 'v', 'gpus', 'ai tooling.', 'dashboards', 'numpy', 'vinai autopilot', 'pandas', 'h', 'vin b']\", 0.34073367118835446]\n",
      "[74, 'All-round Engineer', \"['ue', 'laravel engine', 'agile', 'js', 'search engine', 'php', 'python', '4', 'batch']\", 0.32163201570510863]\n",
      "[92, 'Software Engineer, Observability', \"['react', 'grab', 'elk', 'jaeger', 'python', 'nodejs', 'prometheus', 'amazon web services', 'observability', 'go', 'java', 'aws', 'unified observability', 'reduxflow', 'angular', 'golang']\", 0.3089586853981018]\n",
      "[127, 'AI Engineer, Kobiton', \"['js', 'backend', 'mobiles', 'vietnam', 'java', 'mobile', 'desktops', 'postgredb', 'nguyen', 'ptim', 'k8s', 'golang', 'ghn express', 'docker', 'tablets', 'node', '0906738', 'mob', 'restful api', 'back', 'telegram', 'mongodb', 'p']\", 0.30338029861450194]\n",
      "[138, 'AI Engineer, Kobiton', \"['rbvh', 'hcmc', 'c++', 'c', 'sap', 'python union', 'robert bosch gmbh', 'jav', 'robert bosch', 'matlab', 'microcontroller']\", 0.30338029861450194]\n",
      "[129, 'Fresh Software Engineer', \"['orm', 'win', 'client', 'kms', 'c#', 'javascript', '19', '.net', 'wp', 'upstar labs', 'scrum', 'mysql', 'entity', 'cov', 'sql', 'asp.net mvc', 'nhibernate', 'microsoft sql server', 'qasymph', 'winform', 'web api']\", 0.26918667554855347]\n",
      "[89, 'Bridge Software Engineer (BSE)', \"['c++', 'ho', 'c', '.net', 'project manager', 'java', 'ha']\", 0.2661460340023041]\n",
      "[79, 'Full Stack Software Engineer (.NET, C#)', \"['reactjs', 'html', 'asp.net core', 'vs', '8:00 âģĵ 10: 00 am', 'azure devops', 'c#', 'javascript', 'git', 'hub', 'sql', '.net', 'dan', 'agile', 'angular']\", 0.26532386541366576]\n",
      "[0, 'Fresher Python Software Engineer', \"['client', 'kms', '19', 'cov-', 'k', 'upstar labs']\", 0.2651121497154236]\n",
      "[113, '[ECM] Test Engineer', \"['vuejs', 'loopback', 'react', 'ui-ux', 'kms', 'express', 'nodejs', 'kobiton', 'tricentis', 'angularjs', '19emic', 'katalon', 'k', '($100m', 'mobile', 'covid', 'koa', 'qasymphony']\", 0.26384042501449584]\n"
     ]
    }
   ],
   "source": [
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnaipE9_E3ns"
   },
   "outputs": [],
   "source": [
    "# course_titles = ['Security in Computing and Information Technology']\n",
    "course_titles = ['Peer-to-Peer Networks','Distributed Systems']\n",
    "results = my_recommender.recommend(course_titles, job_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gq2TUjZWFhwn",
    "outputId": "1a0c2460-cb2c-4525-8dc3-a9bc172ac3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92, 'Software Engineer, Observability', \"['react', 'grab', 'elk', 'jaeger', 'python', 'nodejs', 'prometheus', 'amazon web services', 'observability', 'go', 'java', 'aws', 'unified observability', 'reduxflow', 'angular', 'golang']\", 0.29281051754951476]\n",
      "[78, 'SOFTWARE ENGINEER (HCM)', \"['$700', 'devops', 'data engineer', '15000000.', 'vue.js', 'relational', 'java', '. 40000000', 'c¦ung', 'data analyst', 'angular.js', 'ch', 'v', '24-09-2021', 'yãĭu', 'sql', 'jvm', '$2000', 'python', 'react.js', 'scala']\", 0.2801404237747192]\n",
      "[36, 'System/Network Engineer_Freshers', \"['hypervisors', '3 switch', 'shell', 'jun', 'super computer', 'vmware', 'qemui', 'vlan', 'linux', 'kvm', 'hyper-v', 'l', 'hpc']\", 0.2568916082382202]\n",
      "[113, '[ECM] Test Engineer', \"['vuejs', 'loopback', 'react', 'ui-ux', 'kms', 'express', 'nodejs', 'kobiton', 'tricentis', 'angularjs', '19emic', 'katalon', 'k', '($100m', 'mobile', 'covid', 'koa', 'qasymphony']\", 0.2493357181549072]\n",
      "[77, 'Software Engineer in Data Science', \"['google deepmind', 'autopilot', 'autopilot ai', 'vinai', 'python', 'v', 'gpus', 'ai tooling.', 'dashboards', 'numpy', 'vinai autopilot', 'pandas', 'h', 'vin b']\", 0.24794490337371825]\n",
      "[103, 'Software Engineer', \"['5', 'html', 'client', 'cocos creator', 'cocos', 'javascript', 'css', '2dx', 'restful api', 'unity', 'mobile', 'type']\", 0.24549003839492797]\n",
      "[47, 'Fullstack Software Engineer (ReactJS/NodeJS/Go)', \"['n', 'svn', 'webpack', 'javascript', 'git', 'es', '6', 'bower', 'reactjs', 'golang', 'babel', 'chotot', 'docker', 'web browser', 'nodejs', 'mac', 'json', 'grunt', 'full stack']\", 0.2437905728816986]\n",
      "[18, 'Software Engineer (Golang/Java)', \"['server', 'backend', 'tidb', 'java', 'container', 'javascript', 'rdbms', 'git', 'css', 'angularjs', 'reactjs', 'golang', 'html', 'docker', 'mysql', 'redis', 'kafka', 'nosql', 'zalopay', 'frontend', 'jquery']\", 0.24373272657394407]\n",
      "[34, 'Automation Test Engineer', \"['mocha', 'kms', 'appium', 'testng', 'kms labsasymph', 'nightwatch', 'jasmine', 'scrum', 'selenium']\", 0.23755048513412474]\n",
      "[104, 'Software Engineer', \"['microsoft .net', 'sql server']\", 0.23728473186492918]\n"
     ]
    }
   ],
   "source": [
    "show_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBQO258mxyns"
   },
   "outputs": [],
   "source": [
    "# Get skills learned from courses\n",
    "my_skills = get_skills_from_course_titles(course_titles)\n",
    "my_skills"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP: Recommendation System CS-IT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
